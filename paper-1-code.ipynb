{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12466764,"sourceType":"datasetVersion","datasetId":7864845}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Bangla Sentiment Analysis using Bangla-BERT-base\n\nThis notebook implements the best performing model from the paper \"Bangla Sentiment Analysis On Highly Imbalanced Data Using Hybrid CNN-LSTM & Bangla BERT\"\n\nThe Bangla-BERT-base model achieved 96% accuracy with 10-fold cross-validation.","metadata":{}},{"cell_type":"markdown","source":"## 1. Install Required Libraries","metadata":{}},{"cell_type":"code","source":"!pip install transformers torch scikit-learn pandas numpy tqdm matplotlib seaborn imbalanced-learn","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Import Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForSequenceClassification,\n    AdamW,\n    get_linear_schedule_with_warmup\n)\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.metrics import (\n    accuracy_score, \n    f1_score, \n    precision_score, \n    recall_score,\n    confusion_matrix,\n    roc_auc_score\n)\nfrom imblearn.over_sampling import SMOTE\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {device}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Data Preprocessing Functions","metadata":{}},{"cell_type":"code","source":"def preprocess_for_bert(text):\n    \"\"\"\n    Minimal preprocessing for BERT model\n    According to the paper, for BERT they only removed URLs\n    \"\"\"\n    # Remove URLs\n    text = re.sub(r'http\\S+|www.\\S+', '', text)\n    \n    # Remove extra whitespaces\n    text = ' '.join(text.split())\n    \n    return text.strip()\n\ndef load_and_preprocess_data(file_path):\n    \"\"\"\n    Load and preprocess the dataset\n    Expected format: CSV with 'Text' and 'Polarity' columns\n    Polarity values: 'positive', 'negative', 'neutral'\n    Neutral samples will be dropped\n    \"\"\"\n    # Load data\n    df = pd.read_csv(file_path)\n    \n    # Ensure we have the required columns\n    if 'Text' not in df.columns or 'Polarity' not in df.columns:\n        raise ValueError(\"Dataset must have 'Text' and 'Polarity' columns\")\n    \n    # Rename columns to match expected format\n    df = df.rename(columns={'Text': 'text', 'Polarity': 'label'})\n    \n    # Print original distribution\n    print(\"Original dataset distribution:\")\n    print(df['label'].value_counts())\n    print()\n    \n    # Drop neutral samples\n    df = df[df['label'] != 'neutral']\n    print(f\"After dropping neutral samples: {len(df)} samples remaining\")\n    \n    # Convert polarity to binary labels\n    # positive -> 1, negative -> 0\n    df['label'] = df['label'].map({'positive': 1, 'negative': 0})\n    \n    # Drop any rows with NaN labels (in case of unexpected values)\n    df = df.dropna(subset=['label'])\n    \n    # Convert label to int\n    df['label'] = df['label'].astype(int)\n    \n    # Apply preprocessing\n    df['text'] = df['text'].apply(preprocess_for_bert)\n    \n    # Remove any empty texts\n    df = df[df['text'].str.len() > 0]\n    \n    # Print dataset statistics\n    print(f\"\\nFinal dataset statistics:\")\n    print(f\"Total samples: {len(df)}\")\n    print(f\"Positive samples: {len(df[df['label'] == 1])} ({len(df[df['label'] == 1])/len(df)*100:.1f}%)\")\n    print(f\"Negative samples: {len(df[df['label'] == 0])} ({len(df[df['label'] == 0])/len(df)*100:.1f}%)\")\n    \n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Dataset Class for BERT","metadata":{}},{"cell_type":"code","source":"class BanglaSentimentDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        label = self.labels[idx]\n        \n        encoding = self.tokenizer(\n            text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        \n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Model Training and Evaluation Functions","metadata":{}},{"cell_type":"code","source":"def train_epoch(model, dataloader, optimizer, scheduler, device):\n    model.train()\n    total_loss = 0\n    predictions = []\n    true_labels = []\n    \n    for batch in tqdm(dataloader, desc='Training'):\n        optimizer.zero_grad()\n        \n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        \n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n        \n        loss = outputs.loss\n        logits = outputs.logits\n        \n        total_loss += loss.item()\n        \n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n        \n        preds = torch.argmax(logits, dim=-1)\n        predictions.extend(preds.cpu().numpy())\n        true_labels.extend(labels.cpu().numpy())\n    \n    avg_loss = total_loss / len(dataloader)\n    accuracy = accuracy_score(true_labels, predictions)\n    \n    return avg_loss, accuracy\n\ndef evaluate(model, dataloader, device):\n    model.eval()\n    total_loss = 0\n    predictions = []\n    true_labels = []\n    probabilities = []\n    \n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc='Evaluating'):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            \n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                labels=labels\n            )\n            \n            loss = outputs.loss\n            logits = outputs.logits\n            \n            total_loss += loss.item()\n            \n            probs = torch.softmax(logits, dim=-1)\n            preds = torch.argmax(logits, dim=-1)\n            \n            predictions.extend(preds.cpu().numpy())\n            true_labels.extend(labels.cpu().numpy())\n            probabilities.extend(probs.cpu().numpy())\n    \n    avg_loss = total_loss / len(dataloader)\n    accuracy = accuracy_score(true_labels, predictions)\n    f1 = f1_score(true_labels, predictions, average='weighted')\n    f1_neg = f1_score(true_labels, predictions, pos_label=0, average='binary')\n    f1_pos = f1_score(true_labels, predictions, pos_label=1, average='binary')\n    precision = precision_score(true_labels, predictions, average='weighted')\n    recall = recall_score(true_labels, predictions, average='weighted')\n    \n    # Calculate ROC AUC for binary classification\n    probabilities = np.array(probabilities)\n    roc_auc = roc_auc_score(true_labels, probabilities[:, 1])\n    \n    metrics = {\n        'loss': avg_loss,\n        'accuracy': accuracy,\n        'f1': f1,\n        'f1_negative': f1_neg,\n        'f1_positive': f1_pos,\n        'precision': precision,\n        'recall': recall,\n        'roc_auc': roc_auc\n    }\n    \n    return metrics, predictions, true_labels","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. K-Fold Cross Validation Training","metadata":{}},{"cell_type":"code","source":"def train_with_kfold(df, n_splits=10, batch_size=16, num_epochs=10, use_smote=True):\n    \"\"\"\n    Train Bangla-BERT-base using K-fold cross validation\n    \"\"\"\n    # Initialize tokenizer and load model name\n    model_name = 'sagorsarker/bangla-bert-base'\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    \n    # Prepare data\n    texts = df['text'].values\n    labels = df['label'].values\n    \n    # K-fold cross validation\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n    \n    fold_metrics = []\n    all_confusion_matrices = []\n    \n    for fold, (train_idx, val_idx) in enumerate(skf.split(texts, labels)):\n        print(f\"\\n{'='*50}\")\n        print(f\"Fold {fold + 1}/{n_splits}\")\n        print(f\"{'='*50}\")\n        \n        # Split data\n        X_train, X_val = texts[train_idx], texts[val_idx]\n        y_train, y_val = labels[train_idx], labels[val_idx]\n        \n        # Apply SMOTE if specified\n        if use_smote:\n            print(\"Applying SMOTE oversampling...\")\n            # Create a temporary dataframe for SMOTE\n            train_df = pd.DataFrame({'text': X_train, 'label': y_train})\n            \n            # For SMOTE, we need numeric features, so we'll use indices\n            indices = np.arange(len(X_train)).reshape(-1, 1)\n            smote = SMOTE(random_state=42)\n            indices_resampled, y_train_resampled = smote.fit_resample(indices, y_train)\n            \n            # Get the resampled texts\n            X_train_resampled = []\n            for idx in indices_resampled.flatten():\n                X_train_resampled.append(X_train[idx])\n            \n            X_train = np.array(X_train_resampled)\n            y_train = y_train_resampled\n            \n            print(f\"After SMOTE - Train samples: {len(X_train)}\")\n        \n        # Create datasets\n        train_dataset = BanglaSentimentDataset(X_train, y_train, tokenizer)\n        val_dataset = BanglaSentimentDataset(X_val, y_val, tokenizer)\n        \n        # Create dataloaders\n        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n        val_loader = DataLoader(val_dataset, batch_size=batch_size*2, shuffle=False)\n        \n        # Initialize model\n        model = AutoModelForSequenceClassification.from_pretrained(\n            model_name,\n            num_labels=2\n        ).to(device)\n        \n        # Setup optimizer and scheduler\n        optimizer = AdamW(model.parameters(), lr=1e-5, eps=1e-8)\n        \n        num_training_steps = len(train_loader) * num_epochs\n        num_warmup_steps = int(0.1 * num_training_steps)  # 10% warmup\n        \n        scheduler = get_linear_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=num_warmup_steps,\n            num_training_steps=num_training_steps\n        )\n        \n        # Training loop\n        best_val_f1 = 0\n        best_model_state = None\n        \n        for epoch in range(num_epochs):\n            print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n            \n            # Train\n            train_loss, train_acc = train_epoch(model, train_loader, optimizer, scheduler, device)\n            \n            # Evaluate\n            val_metrics, val_preds, val_labels = evaluate(model, val_loader, device)\n            \n            print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n            print(f\"Val Loss: {val_metrics['loss']:.4f}, Val Acc: {val_metrics['accuracy']:.4f}\")\n            print(f\"Val F1: {val_metrics['f1']:.4f} (Neg: {val_metrics['f1_negative']:.4f}, Pos: {val_metrics['f1_positive']:.4f})\")\n            \n            # Save best model\n            if val_metrics['f1'] > best_val_f1:\n                best_val_f1 = val_metrics['f1']\n                best_model_state = model.state_dict()\n                print(f\"✓ New best model! F1: {best_val_f1:.4f}\")\n        \n        # Load best model and get final metrics\n        model.load_state_dict(best_model_state)\n        final_metrics, final_preds, final_labels = evaluate(model, val_loader, device)\n        \n        # Store metrics\n        fold_metrics.append(final_metrics)\n        \n        # Calculate confusion matrix\n        cm = confusion_matrix(final_labels, final_preds)\n        all_confusion_matrices.append(cm)\n        \n        print(f\"\\nFold {fold + 1} Final Metrics:\")\n        for key, value in final_metrics.items():\n            print(f\"{key}: {value:.4f}\")\n    \n    return fold_metrics, all_confusion_matrices","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Results Visualization Functions","metadata":{}},{"cell_type":"code","source":"def plot_average_confusion_matrix(confusion_matrices, save_path=None):\n    \"\"\"\n    Plot average confusion matrix from k-fold cross validation\n    \"\"\"\n    # Calculate average confusion matrix\n    avg_cm = np.mean(confusion_matrices, axis=0)\n    \n    plt.figure(figsize=(8, 6))\n    sns.heatmap(avg_cm, annot=True, fmt='.2f', cmap='Blues', \n                xticklabels=['Negative', 'Positive'],\n                yticklabels=['Negative', 'Positive'])\n    plt.title('Average Confusion Matrix', fontsize=16)\n    plt.ylabel('True Labels', fontsize=12)\n    plt.xlabel('Predicted Labels', fontsize=12)\n    \n    if save_path:\n        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    plt.show()\n\ndef print_final_results(fold_metrics):\n    \"\"\"\n    Print final averaged results with standard deviation\n    \"\"\"\n    # Convert to numpy array for easier calculation\n    metrics_array = {key: [] for key in fold_metrics[0].keys()}\n    \n    for fold_metric in fold_metrics:\n        for key, value in fold_metric.items():\n            metrics_array[key].append(value)\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"FINAL RESULTS - Mean ± Standard Deviation\")\n    print(\"=\"*60)\n    \n    for key, values in metrics_array.items():\n        mean_val = np.mean(values)\n        std_val = np.std(values)\n        print(f\"{key.capitalize():15s}: {mean_val:.4f} ± {std_val:.4f}\")\n    \n    # Create results dataframe\n    results_df = pd.DataFrame(metrics_array)\n    results_df['fold'] = range(1, len(fold_metrics) + 1)\n    \n    return results_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8. Main Training Pipeline","metadata":{}},{"cell_type":"code","source":"# Load your dataset\n# Replace 'your_dataset.csv' with your actual file path\n# The CSV should have 'text' and 'label' columns\ndf = load_and_preprocess_data('/kaggle/input/final-dataset/final-dataset.csv')\n\n# Train with 10-fold cross validation (as per the paper)\nfold_metrics, confusion_matrices = train_with_kfold(\n    df, \n    n_splits=10,\n    batch_size=16,\n    num_epochs=10,\n    use_smote=True  # Paper mentions using SMOTE\n)\n\n# Print final results\nresults_df = print_final_results(fold_metrics)\n\n# Plot average confusion matrix\nplot_average_confusion_matrix(confusion_matrices, save_path='average_confusion_matrix.png')\n\n# Save results to CSV\nresults_df.to_csv('bangla_bert_results.csv', index=False)\nprint(\"\\nResults saved to 'bangla_bert_results.csv'\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9. Single Model Training (Hold-out Method)","metadata":{}},{"cell_type":"code","source":"def train_single_model(df, test_size=0.2, batch_size=16, num_epochs=10, use_smote=True):\n    \"\"\"\n    Train a single model with train-test split (hold-out method)\n    \"\"\"\n    # Split data\n    X = df['text'].values\n    y = df['label'].values\n    \n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=test_size, stratify=y, random_state=42\n    )\n    \n    print(f\"Train samples: {len(X_train)}\")\n    print(f\"Test samples: {len(X_test)}\")\n    \n    # Apply SMOTE if specified\n    if use_smote:\n        print(\"\\nApplying SMOTE oversampling...\")\n        indices = np.arange(len(X_train)).reshape(-1, 1)\n        smote = SMOTE(random_state=42)\n        indices_resampled, y_train_resampled = smote.fit_resample(indices, y_train)\n        \n        X_train_resampled = []\n        for idx in indices_resampled.flatten():\n            X_train_resampled.append(X_train[idx])\n        \n        X_train = np.array(X_train_resampled)\n        y_train = y_train_resampled\n        print(f\"After SMOTE - Train samples: {len(X_train)}\")\n    \n    # Initialize tokenizer and model\n    model_name = 'sagorsarker/bangla-bert-base'\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSequenceClassification.from_pretrained(\n        model_name,\n        num_labels=2\n    ).to(device)\n    \n    # Create datasets and dataloaders\n    train_dataset = BanglaSentimentDataset(X_train, y_train, tokenizer)\n    test_dataset = BanglaSentimentDataset(X_test, y_test, tokenizer)\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size*2, shuffle=False)\n    \n    # Setup optimizer and scheduler\n    optimizer = AdamW(model.parameters(), lr=1e-5, eps=1e-8)\n    \n    num_training_steps = len(train_loader) * num_epochs\n    num_warmup_steps = int(0.1 * num_training_steps)\n    \n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=num_warmup_steps,\n        num_training_steps=num_training_steps\n    )\n    \n    # Training loop\n    best_val_f1 = 0\n    best_model_state = None\n    train_losses = []\n    val_losses = []\n    \n    for epoch in range(num_epochs):\n        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n        \n        # Train\n        train_loss, train_acc = train_epoch(model, train_loader, optimizer, scheduler, device)\n        train_losses.append(train_loss)\n        \n        # Evaluate\n        val_metrics, _, _ = evaluate(model, test_loader, device)\n        val_losses.append(val_metrics['loss'])\n        \n        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n        print(f\"Test Loss: {val_metrics['loss']:.4f}, Test Acc: {val_metrics['accuracy']:.4f}\")\n        print(f\"Test F1: {val_metrics['f1']:.4f} (Neg: {val_metrics['f1_negative']:.4f}, Pos: {val_metrics['f1_positive']:.4f})\")\n        \n        # Save best model\n        if val_metrics['f1'] > best_val_f1:\n            best_val_f1 = val_metrics['f1']\n            best_model_state = model.state_dict()\n            print(f\"✓ New best model! F1: {best_val_f1:.4f}\")\n    \n    # Load best model\n    model.load_state_dict(best_model_state)\n    \n    # Final evaluation\n    final_metrics, predictions, true_labels = evaluate(model, test_loader, device)\n    \n    print(\"\\n\" + \"=\"*50)\n    print(\"FINAL TEST RESULTS\")\n    print(\"=\"*50)\n    for key, value in final_metrics.items():\n        print(f\"{key.capitalize():15s}: {value:.4f}\")\n    \n    # Plot confusion matrix\n    cm = confusion_matrix(true_labels, predictions)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=['Negative', 'Positive'],\n                yticklabels=['Negative', 'Positive'])\n    plt.title('Confusion Matrix', fontsize=16)\n    plt.ylabel('True Labels', fontsize=12)\n    plt.xlabel('Predicted Labels', fontsize=12)\n    plt.savefig('confusion_matrix_single_model.png', dpi=300, bbox_inches='tight')\n    plt.show()\n    \n    # Plot training history\n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss')\n    plt.plot(range(1, num_epochs + 1), val_losses, label='Test Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Training History')\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n    plt.show()\n    \n    return model, tokenizer, final_metrics\n\n# Uncomment to train a single model\n# model, tokenizer, metrics = train_single_model(df, test_size=0.2, use_smote=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 10. Save and Load Trained Model","metadata":{}},{"cell_type":"code","source":"def save_model(model, tokenizer, save_path='bangla_bert_sentiment_model'):\n    \"\"\"\n    Save the trained model and tokenizer\n    \"\"\"\n    model.save_pretrained(save_path)\n    tokenizer.save_pretrained(save_path)\n    print(f\"Model saved to {save_path}\")\n\ndef load_model(model_path='bangla_bert_sentiment_model'):\n    \"\"\"\n    Load a saved model and tokenizer\n    \"\"\"\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)\n    print(f\"Model loaded from {model_path}\")\n    return model, tokenizer\n\n# Example usage:\n# save_model(model, tokenizer)\n# loaded_model, loaded_tokenizer = load_model()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 11. Inference Function for New Texts","metadata":{}},{"cell_type":"code","source":"def predict_sentiment(text, model, tokenizer):\n    \"\"\"\n    Predict sentiment for a single text\n    \"\"\"\n    # Preprocess text\n    text = preprocess_for_bert(text)\n    \n    # Tokenize\n    encoding = tokenizer(\n        text,\n        truncation=True,\n        padding='max_length',\n        max_length=128,\n        return_tensors='pt'\n    )\n    \n    # Move to device\n    input_ids = encoding['input_ids'].to(device)\n    attention_mask = encoding['attention_mask'].to(device)\n    \n    # Get prediction\n    model.eval()\n    with torch.no_grad():\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        probs = torch.softmax(logits, dim=-1)\n        prediction = torch.argmax(logits, dim=-1)\n    \n    # Get sentiment and confidence\n    sentiment = 'Positive' if prediction.item() == 1 else 'Negative'\n    confidence = probs[0][prediction.item()].item()\n    \n    return {\n        'sentiment': sentiment,\n        'confidence': confidence,\n        'probabilities': {\n            'negative': probs[0][0].item(),\n            'positive': probs[0][1].item()\n        }\n    }\n\n# Example usage:\n# result = predict_sentiment(\"আমি খুব খুশি\", model, tokenizer)\n# print(result)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 12. Batch Prediction Function","metadata":{}},{"cell_type":"code","source":"def predict_batch(texts, model, tokenizer, batch_size=32):\n    \"\"\"\n    Predict sentiments for multiple texts\n    \"\"\"\n    # Preprocess texts\n    processed_texts = [preprocess_for_bert(text) for text in texts]\n    \n    # Create dummy labels (required for dataset)\n    dummy_labels = np.zeros(len(texts))\n    \n    # Create dataset and dataloader\n    dataset = BanglaSentimentDataset(processed_texts, dummy_labels, tokenizer)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n    \n    # Predictions\n    all_predictions = []\n    all_probabilities = []\n    \n    model.eval()\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc='Predicting'):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            \n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            logits = outputs.logits\n            probs = torch.softmax(logits, dim=-1)\n            preds = torch.argmax(logits, dim=-1)\n            \n            all_predictions.extend(preds.cpu().numpy())\n            all_probabilities.extend(probs.cpu().numpy())\n    \n    # Format results\n    results = []\n    for i, (text, pred, prob) in enumerate(zip(texts, all_predictions, all_probabilities)):\n        results.append({\n            'text': text,\n            'sentiment': 'Positive' if pred == 1 else 'Negative',\n            'confidence': prob[pred],\n            'probabilities': {\n                'negative': prob[0],\n                'positive': prob[1]\n            }\n        })\n    \n    return results\n\n# Example usage:\n# texts = [\"আমি খুব খুশি\", \"এটা খুব খারাপ\", \"মোটামুটি ভালো\"]\n# results = predict_batch(texts, model, tokenizer)\n# for result in results:\n#     print(f\"Text: {result['text']}\")\n#     print(f\"Sentiment: {result['sentiment']} (Confidence: {result['confidence']:.4f})\")\n#     print()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}