{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12250103,"sourceType":"datasetVersion","datasetId":7718676},{"sourceId":12467039,"sourceType":"datasetVersion","datasetId":7865054}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n","metadata":{}},{"cell_type":"code","source":"!pip install transformers torch pandas scikit-learn numpy tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T02:14:44.366432Z","iopub.execute_input":"2025-07-23T02:14:44.366652Z","iopub.status.idle":"2025-07-23T02:16:06.640472Z","shell.execute_reply.started":"2025-07-23T02:14:44.366628Z","shell.execute_reply":"2025-07-23T02:16:06.639736Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.4)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nimport numpy as np\nfrom transformers import XLMRobertaTokenizer, XLMRobertaModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T02:16:06.642403Z","iopub.execute_input":"2025-07-23T02:16:06.642630Z","iopub.status.idle":"2025-07-23T02:16:32.593076Z","shell.execute_reply.started":"2025-07-23T02:16:06.642606Z","shell.execute_reply":"2025-07-23T02:16:32.592290Z"}},"outputs":[{"name":"stderr","text":"2025-07-23 02:16:20.251319: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753236980.448089      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753236980.505242      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Set random seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T02:16:32.594107Z","iopub.execute_input":"2025-07-23T02:16:32.594804Z","iopub.status.idle":"2025-07-23T02:16:32.605337Z","shell.execute_reply.started":"2025-07-23T02:16:32.594750Z","shell.execute_reply":"2025-07-23T02:16:32.604654Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"class BanglaTextDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        label = self.labels[idx]\n        \n        # Tokenize text\n        encoding = self.tokenizer(\n            text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        \n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'label': torch.tensor(label, dtype=torch.long)\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T02:16:32.606119Z","iopub.execute_input":"2025-07-23T02:16:32.606419Z","iopub.status.idle":"2025-07-23T02:16:33.917268Z","shell.execute_reply.started":"2025-07-23T02:16:32.606396Z","shell.execute_reply":"2025-07-23T02:16:33.916335Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class XLMRobertaForTextClassification(nn.Module):\n    def __init__(self, num_classes=3, model_name='xlm-roberta-large'):\n        super(XLMRobertaForTextClassification, self).__init__()\n        \n        # Load pre-trained XLM-RoBERTa model\n        self.roberta = XLMRobertaModel.from_pretrained(model_name)\n        \n        # Get hidden size from config\n        hidden_size = self.roberta.config.hidden_size\n        \n        # Classification head (as described in the paper for RoBERTa)\n        # Hidden layer with tanh activation followed by classification layer\n        self.classifier = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.Tanh(),\n            nn.Linear(hidden_size, num_classes)\n        )\n        \n    def forward(self, input_ids, attention_mask):\n        # Get RoBERTa outputs\n        outputs = self.roberta(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n        \n        # Get the [CLS] token representation (first token)\n        cls_output = outputs.last_hidden_state[:, 0, :]\n        \n        # Pass through classification head\n        logits = self.classifier(cls_output)\n        \n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T02:16:33.919336Z","iopub.execute_input":"2025-07-23T02:16:33.919558Z","iopub.status.idle":"2025-07-23T02:16:33.932046Z","shell.execute_reply.started":"2025-07-23T02:16:33.919542Z","shell.execute_reply":"2025-07-23T02:16:33.931329Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Load your dataset\n# Replace 'your_dataset.csv' with your actual file path\ndf = pd.read_csv('/kaggle/input/final-dataset/final-dataset.csv')\n\n# Map labels to integers\nlabel_map = {'positive': 0, 'negative': 1, 'neutral': 2}\ndf['label_encoded'] = df['Polarity'].map(label_map)  # Changed from 'Label' to 'Polarity'\n\n# Check if mapping was successful\nif df['label_encoded'].isnull().any():\n    print(\"Warning: Some labels couldn't be mapped. Unique values in Polarity column:\")\n    print(df['Polarity'].unique())\n    # Handle any case sensitivity issues\n    df['Polarity'] = df['Polarity'].str.lower().str.strip()\n    df['label_encoded'] = df['Polarity'].map(label_map)\n\n# Split the data (80% train, 10% validation, 10% test)\ntexts = df['Text'].values  # Text column remains the same\nlabels = df['label_encoded'].values\n\n# First split: 80% train+val, 20% test\nX_temp, X_test, y_temp, y_test = train_test_split(\n    texts, labels, test_size=0.1, random_state=42, stratify=labels\n)\n\n# Second split: 90% train, 10% val (from the 90% temp)\nX_train, X_val, y_train, y_val = train_test_split(\n    X_temp, y_temp, test_size=0.111, random_state=42, stratify=y_temp\n)\n\nprint(f\"Train size: {len(X_train)}\")\nprint(f\"Validation size: {len(X_val)}\")\nprint(f\"Test size: {len(X_test)}\")\n\n# Print label distribution\nprint(\"\\nLabel distribution in training set:\")\nunique, counts = np.unique(y_train, return_counts=True)\nfor label, count in zip(unique, counts):\n    label_name = [k for k, v in label_map.items() if v == label][0]\n    print(f\"{label_name}: {count} ({count/len(y_train)*100:.2f}%)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T02:16:33.932893Z","iopub.execute_input":"2025-07-23T02:16:33.933181Z","iopub.status.idle":"2025-07-23T02:16:34.046436Z","shell.execute_reply.started":"2025-07-23T02:16:33.933157Z","shell.execute_reply":"2025-07-23T02:16:34.045496Z"}},"outputs":[{"name":"stdout","text":"Train size: 4944\nValidation size: 618\nTest size: 618\n\nLabel distribution in training set:\npositive: 1638 (33.13%)\nnegative: 1581 (31.98%)\nneutral: 1725 (34.89%)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Initialize tokenizer\ntokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-large')\n\n# Set max length based on your dataset\n# The paper mentions different lengths for different datasets\n# For sentiment analysis on short texts, they used 30-100 tokens\nmax_length = 100  # Adjust based on your text length\n\n# Create datasets\ntrain_dataset = BanglaTextDataset(X_train, y_train, tokenizer, max_length)\nval_dataset = BanglaTextDataset(X_val, y_val, tokenizer, max_length)\ntest_dataset = BanglaTextDataset(X_test, y_test, tokenizer, max_length)\n\n# Create data loaders\n# Paper mentions batch size of 32\nbatch_size = 32  # Reduce if you run into memory issues\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T02:16:34.047493Z","iopub.execute_input":"2025-07-23T02:16:34.047916Z","iopub.status.idle":"2025-07-23T02:16:38.441622Z","shell.execute_reply.started":"2025-07-23T02:16:34.047882Z","shell.execute_reply":"2025-07-23T02:16:38.440715Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91a280c35a5346858cdef544c0bd56dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08a3bf8bf12e4fd89028ead9cd834a4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5a7f1a330e54fdfad618e45704be6a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/616 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70ed93e4a6f04999845225bac6601ad4"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# Initialize model\nmodel = XLMRobertaForTextClassification(num_classes=3)\nmodel = model.to(device)\n\n# Loss function\ncriterion = nn.CrossEntropyLoss()\n\n# Optimizer (Adam with learning rate 1e-5 as mentioned in the paper)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n\n# Number of epochs (paper mentions 10 epochs)\nnum_epochs = 10","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T02:16:38.442532Z","iopub.execute_input":"2025-07-23T02:16:38.442790Z","iopub.status.idle":"2025-07-23T02:16:51.082034Z","shell.execute_reply.started":"2025-07-23T02:16:38.442770Z","shell.execute_reply":"2025-07-23T02:16:51.081086Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15b4d81e694047228101ab750fce934c"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"def train_epoch(model, data_loader, criterion, optimizer, device):\n    model.train()\n    total_loss = 0\n    predictions = []\n    actual_labels = []\n    \n    for batch in tqdm(data_loader, desc=\"Training\"):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label'].to(device)\n        \n        optimizer.zero_grad()\n        \n        outputs = model(input_ids, attention_mask)\n        loss = criterion(outputs, labels)\n        \n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        \n        _, preds = torch.max(outputs, dim=1)\n        predictions.extend(preds.cpu().numpy())\n        actual_labels.extend(labels.cpu().numpy())\n    \n    avg_loss = total_loss / len(data_loader)\n    accuracy = accuracy_score(actual_labels, predictions)\n    f1 = f1_score(actual_labels, predictions, average='weighted')\n    \n    return avg_loss, accuracy, f1\n\ndef evaluate(model, data_loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    predictions = []\n    actual_labels = []\n    \n    with torch.no_grad():\n        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n            \n            outputs = model(input_ids, attention_mask)\n            loss = criterion(outputs, labels)\n            \n            total_loss += loss.item()\n            \n            _, preds = torch.max(outputs, dim=1)\n            predictions.extend(preds.cpu().numpy())\n            actual_labels.extend(labels.cpu().numpy())\n    \n    avg_loss = total_loss / len(data_loader)\n    accuracy = accuracy_score(actual_labels, predictions)\n    f1 = f1_score(actual_labels, predictions, average='weighted')\n    \n    return avg_loss, accuracy, f1, predictions, actual_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T02:16:51.082849Z","iopub.execute_input":"2025-07-23T02:16:51.083185Z","iopub.status.idle":"2025-07-23T02:16:51.096131Z","shell.execute_reply.started":"2025-07-23T02:16:51.083153Z","shell.execute_reply":"2025-07-23T02:16:51.095419Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Training loop\nbest_val_accuracy = 0\nbest_model_path = 'best_xlm_roberta_bangla.pt'\n\nfor epoch in range(num_epochs):\n    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n    print(\"-\" * 50)\n    \n    # Train\n    train_loss, train_acc, train_f1 = train_epoch(\n        model, train_loader, criterion, optimizer, device\n    )\n    print(f\"Train Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}, F1: {train_f1:.4f}\")\n    \n    # Validate\n    val_loss, val_acc, val_f1, _, _ = evaluate(\n        model, val_loader, criterion, device\n    )\n    print(f\"Val Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}, F1: {val_f1:.4f}\")\n    \n    # Save best model based on validation accuracy\n    if val_acc > best_val_accuracy:\n        best_val_accuracy = val_acc\n        torch.save(model.state_dict(), best_model_path)\n        print(f\"Best model saved with validation accuracy: {val_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T02:16:51.096762Z","iopub.execute_input":"2025-07-23T02:16:51.097052Z","iopub.status.idle":"2025-07-23T03:09:38.637666Z","shell.execute_reply.started":"2025-07-23T02:16:51.097025Z","shell.execute_reply":"2025-07-23T03:09:38.636918Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 1/10\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 155/155 [04:52<00:00,  1.89s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.0416, Accuracy: 0.4581, F1: 0.4443\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 20/20 [00:11<00:00,  1.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.8489, Accuracy: 0.6084, F1: 0.5781\nBest model saved with validation accuracy: 0.6084\n\nEpoch 2/10\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 155/155 [05:02<00:00,  1.95s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7763, Accuracy: 0.6640, F1: 0.6627\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 20/20 [00:11<00:00,  1.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.6867, Accuracy: 0.7023, F1: 0.6936\nBest model saved with validation accuracy: 0.7023\n\nEpoch 3/10\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 155/155 [05:03<00:00,  1.96s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6036, Accuracy: 0.7520, F1: 0.7514\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 20/20 [00:11<00:00,  1.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.6421, Accuracy: 0.7443, F1: 0.7412\nBest model saved with validation accuracy: 0.7443\n\nEpoch 4/10\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 155/155 [05:03<00:00,  1.96s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5142, Accuracy: 0.7933, F1: 0.7929\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 20/20 [00:11<00:00,  1.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.6941, Accuracy: 0.7168, F1: 0.7111\n\nEpoch 5/10\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 155/155 [05:02<00:00,  1.95s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4133, Accuracy: 0.8398, F1: 0.8396\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 20/20 [00:11<00:00,  1.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.7129, Accuracy: 0.7379, F1: 0.7377\n\nEpoch 6/10\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 155/155 [05:02<00:00,  1.95s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3254, Accuracy: 0.8803, F1: 0.8801\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 20/20 [00:11<00:00,  1.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.6863, Accuracy: 0.7670, F1: 0.7665\nBest model saved with validation accuracy: 0.7670\n\nEpoch 7/10\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 155/155 [05:03<00:00,  1.96s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2565, Accuracy: 0.9072, F1: 0.9071\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 20/20 [00:11<00:00,  1.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.6919, Accuracy: 0.7557, F1: 0.7549\n\nEpoch 8/10\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 155/155 [05:03<00:00,  1.96s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2269, Accuracy: 0.9187, F1: 0.9187\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 20/20 [00:11<00:00,  1.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.7505, Accuracy: 0.7848, F1: 0.7844\nBest model saved with validation accuracy: 0.7848\n\nEpoch 9/10\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 155/155 [05:03<00:00,  1.96s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1753, Accuracy: 0.9391, F1: 0.9391\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 20/20 [00:11<00:00,  1.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.9524, Accuracy: 0.7411, F1: 0.7397\n\nEpoch 10/10\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 155/155 [05:02<00:00,  1.95s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1302, Accuracy: 0.9541, F1: 0.9541\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 20/20 [00:11<00:00,  1.73it/s]","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.8336, Accuracy: 0.7718, F1: 0.7717\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Load best model\nmodel.load_state_dict(torch.load(best_model_path))\n\n# Evaluate on test set\ntest_loss, test_acc, test_f1, predictions, actual_labels = evaluate(\n    model, test_loader, criterion, device\n)\n\nprint(f\"\\nTest Results:\")\nprint(f\"Test Loss: {test_loss:.4f}\")\nprint(f\"Test Accuracy: {test_acc:.4f}\")\nprint(f\"Test F1 Score: {test_f1:.4f}\")\n\n# Detailed classification report\nlabel_names = ['positive', 'negative', 'neutral']\nprint(\"\\nClassification Report:\")\nprint(classification_report(actual_labels, predictions, target_names=label_names))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T03:09:38.638680Z","iopub.execute_input":"2025-07-23T03:09:38.638921Z","iopub.status.idle":"2025-07-23T03:09:52.171673Z","shell.execute_reply.started":"2025-07-23T03:09:38.638902Z","shell.execute_reply":"2025-07-23T03:09:52.170994Z"}},"outputs":[{"name":"stderr","text":"Evaluating: 100%|██████████| 20/20 [00:11<00:00,  1.73it/s]","output_type":"stream"},{"name":"stdout","text":"\nTest Results:\nTest Loss: 0.9057\nTest Accuracy: 0.7411\nTest F1 Score: 0.7407\n\nClassification Report:\n              precision    recall  f1-score   support\n\n    positive       0.77      0.80      0.79       205\n    negative       0.76      0.68      0.72       198\n     neutral       0.70      0.73      0.71       215\n\n    accuracy                           0.74       618\n   macro avg       0.74      0.74      0.74       618\nweighted avg       0.74      0.74      0.74       618\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"def predict_sentiment(text, model, tokenizer, device, max_length=100):\n    \"\"\"\n    Predict sentiment for a single text\n    \"\"\"\n    model.eval()\n    \n    # Tokenize\n    encoding = tokenizer(\n        text,\n        truncation=True,\n        padding='max_length',\n        max_length=max_length,\n        return_tensors='pt'\n    )\n    \n    input_ids = encoding['input_ids'].to(device)\n    attention_mask = encoding['attention_mask'].to(device)\n    \n    # Predict\n    with torch.no_grad():\n        outputs = model(input_ids, attention_mask)\n        _, prediction = torch.max(outputs, dim=1)\n    \n    # Map back to label\n    label_map_reverse = {0: 'positive', 1: 'negative', 2: 'neutral'}\n    predicted_label = label_map_reverse[prediction.item()]\n    \n    # Get probabilities\n    probabilities = torch.nn.functional.softmax(outputs, dim=1)\n    \n    return predicted_label, probabilities.cpu().numpy()[0]\n\n# Example usage\nsample_text = \"এই মডেলটি খুব ভালো কাজ করছে\"  # \"This model is working very well\"\npredicted_label, probs = predict_sentiment(sample_text, model, tokenizer, device)\nprint(f\"Text: {sample_text}\")\nprint(f\"Predicted: {predicted_label}\")\nprint(f\"Probabilities - Positive: {probs[0]:.4f}, Negative: {probs[1]:.4f}, Neutral: {probs[2]:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T03:09:52.172584Z","iopub.execute_input":"2025-07-23T03:09:52.172868Z","iopub.status.idle":"2025-07-23T03:09:52.259042Z","shell.execute_reply.started":"2025-07-23T03:09:52.172843Z","shell.execute_reply":"2025-07-23T03:09:52.258425Z"}},"outputs":[{"name":"stdout","text":"Text: এই মডেলটি খুব ভালো কাজ করছে\nPredicted: positive\nProbabilities - Positive: 0.9863, Negative: 0.0011, Neutral: 0.0126\n","output_type":"stream"}],"execution_count":12}]}