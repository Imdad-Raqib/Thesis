{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12250103,"sourceType":"datasetVersion","datasetId":7718676}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Leveraging NLP and ML for Sentiment Analysis of Informal Bangla Text\n## Dataset Details\n\n### Dataset Characteristics\n- **Size**: 3,149 comments\n- **Source**: Web-scraped from multiple social media platforms\n- **Language**: Informal Bangla text (noisy, with dialects and grammatical errors)\n- **Labeling**: Manually labeled by psychology students\n\n### Class Distribution\n| Sentiment | Number of Comments | Percentage |\n|-----------|-------------------|------------|\n| Positive  | 1,197             | 38%        |\n| Negative  | 1,213             | 38.5%      |\n| Neutral   | 739               | 23.5%      |\n\n### Data Columns\n- **Comment**: Raw Bangla text from social media\n- **Sentiment**: Three classes (Positive, Negative, Neutral)\n\n## Best Performing Model\n\n### Model Architecture\n**LSTM (Long Short-Term Memory) with Hyperparameter Tuning**\n\n### Hyperparameters\n| Parameter | Value |\n|-----------|-------|\n| Embedding Dimension | 150 |\n| LSTM Units | 150 |\n| Dropout Rate | 0.2 |\n| Optimizer | RMSprop |\n| Batch Size | 32 |\n| Epochs | 10 |\n\n### Performance Results\n| Dataset Type | Accuracy |\n|--------------|----------|\n| **Informal Bangla Text** | **80.3%** |\n| Formal Bangla Text | 96.9% |\n","metadata":{}},{"cell_type":"markdown","source":"# Cell 1 - Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.utils import to_categorical\nfrom imblearn.over_sampling import SMOTE\nimport matplotlib.pyplot as plt\nimport pickle","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# Load dataset\n","metadata":{}},{"cell_type":"code","source":"# Load your dataset\ndf = pd.read_csv('/kaggle/input/final-dataset/final-dataset.csv')\n  # Replace with your file path\n\n# Display basic information\nprint(f\"Dataset shape: {df.shape}\")\nprint(f\"\\nColumn names: {df.columns.tolist()}\")\nprint(f\"\\nSentiment distribution:\")\nprint(df['Polarity'].value_counts())\nprint(f\"\\nSample data:\")\nprint(df.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 3: Data Preprocessing - Cleaning\n","metadata":{}},{"cell_type":"code","source":"def clean_bangla_text(text):\n    \"\"\"Clean Bangla text according to the paper's preprocessing steps\"\"\"\n    if pd.isna(text):\n        return \"\"\n    \n    # Convert to string\n    text = str(text)\n    \n    # Remove non-Bangla characters (keeping Bangla Unicode range)\n    # Bangla Unicode range: \\u0980-\\u09FF\n    text = re.sub(r'[^\\u0980-\\u09FF\\s]', '', text)\n    \n    # Remove multiple spaces\n    text = re.sub(r'\\s+', ' ', text)\n    \n    # Strip leading/trailing spaces\n    text = text.strip()\n    \n    return text\n\n# Apply cleaning\ndf['Text_cleaned'] = df['Text'].apply(clean_bangla_text)\n\n# Remove empty texts after cleaning\ndf = df[df['Text_cleaned'].str.len() > 0]\n\nprint(f\"Dataset shape after cleaning: {df.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 4: Label Encoding\n","metadata":{}},{"cell_type":"code","source":"# Initialize label encoder\nlabel_encoder = LabelEncoder()\n\n# Fit and transform labels\ndf['Polarity_encoded'] = label_encoder.fit_transform(df['Polarity'])\n\n# Display label mapping\nlabel_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\nprint(\"Label mapping:\")\nfor label, encoded in label_mapping.items():\n    print(f\"{label} -> {encoded}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 5: Handle Class Imbalance using SMOTE\n","metadata":{}},{"cell_type":"code","source":"# First, we need to vectorize the text for SMOTE\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Create TF-IDF vectors for SMOTE\ntfidf_temp = TfidfVectorizer(max_features=5000)\nX_tfidf = tfidf_temp.fit_transform(df['Text_cleaned']).toarray()\ny = df['Polarity_encoded'].values\n\n# Apply SMOTE\nsmote = SMOTE(random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X_tfidf, y)\n\nprint(f\"Original dataset shape: {X_tfidf.shape}\")\nprint(f\"Resampled dataset shape: {X_resampled.shape}\")\nprint(f\"Class distribution after SMOTE: {Counter(y_resampled)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 6: Prepare data for LSTM\n","metadata":{}},{"cell_type":"code","source":"# For LSTM, we'll use the original balanced approach from the paper\n# Split the original data first\nX_train_text, X_test_text, y_train, y_test = train_test_split(\n    df['Text_cleaned'].values, \n    df['Polarity_encoded'].values, \n    test_size=0.2, \n    random_state=42,\n    stratify=df['Polarity_encoded']\n)\n\n# Further split training data for validation\nX_train_text, X_val_text, y_train, y_val = train_test_split(\n    X_train_text, \n    y_train, \n    test_size=0.2, \n    random_state=42,\n    stratify=y_train\n)\n\nprint(f\"Training set size: {len(X_train_text)}\")\nprint(f\"Validation set size: {len(X_val_text)}\")\nprint(f\"Test set size: {len(X_test_text)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 7: Tokenization and Padding\n","metadata":{}},{"cell_type":"code","source":"# Hyperparameters from the paper\nmax_features = 10000  # Maximum number of words to keep\nmax_length = 100      # Maximum sequence length\n\n# Initialize tokenizer\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(X_train_text)\n\n# Convert texts to sequences\nX_train_seq = tokenizer.texts_to_sequences(X_train_text)\nX_val_seq = tokenizer.texts_to_sequences(X_val_text)\nX_test_seq = tokenizer.texts_to_sequences(X_test_text)\n\n# Pad sequences\nX_train_pad = pad_sequences(X_train_seq, maxlen=max_length)\nX_val_pad = pad_sequences(X_val_seq, maxlen=max_length)\nX_test_pad = pad_sequences(X_test_seq, maxlen=max_length)\n\nprint(f\"Padded training shape: {X_train_pad.shape}\")\nprint(f\"Vocabulary size: {len(tokenizer.word_index)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 8: Convert labels to categorical\n","metadata":{}},{"cell_type":"code","source":"num_classes = 3\ny_train_cat = to_categorical(y_train, num_classes)\ny_val_cat = to_categorical(y_val, num_classes)\ny_test_cat = to_categorical(y_test, num_classes)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 9: Build LSTM Model (with hyperparameters from the paper)\n","metadata":{}},{"cell_type":"code","source":"# Hyperparameters from Table 5 in the paper\nembedding_dim = 150\nlstm_units = 150\ndropout_rate = 0.2\n\n# Build model\nmodel = Sequential([\n    Embedding(input_dim=max_features, \n              output_dim=embedding_dim, \n              input_length=max_length),\n    LSTM(lstm_units, return_sequences=False),\n    Dropout(dropout_rate),\n    Dense(num_classes, activation='softmax')\n])\n\n# Compile with RMSprop optimizer as mentioned in the paper\nmodel.compile(\n    optimizer=RMSprop(),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 10: Train the Model\n","metadata":{}},{"cell_type":"code","source":"# Train the model\nhistory = model.fit(\n    X_train_pad, y_train_cat,\n    batch_size=32,\n    epochs=10,\n    validation_data=(X_val_pad, y_val_cat),\n    verbose=1\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 11: Plot Training History\n","metadata":{}},{"cell_type":"code","source":"# Plot accuracy\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Train')\nplt.plot(history.history['val_accuracy'], label='Validation')\nplt.title('Model Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\n\n# Plot loss\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Train')\nplt.plot(history.history['val_loss'], label='Validation')\nplt.title('Model Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 12: Evaluate on Test Set\n","metadata":{}},{"cell_type":"code","source":"# Evaluate on test set\ntest_loss, test_accuracy = model.evaluate(X_test_pad, y_test_cat)\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\nprint(f\"Test Loss: {test_loss:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 13: Get Detailed Predictions\n","metadata":{}},{"cell_type":"code","source":"# Get predictions\ny_pred = model.predict(X_test_pad)\ny_pred_classes = np.argmax(y_pred, axis=1)\n\n# Convert back to original labels\ny_test_labels = label_encoder.inverse_transform(y_test)\ny_pred_labels = label_encoder.inverse_transform(y_pred_classes)\n\n# Create classification report\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nprint(\"Classification Report:\")\nprint(classification_report(y_test_labels, y_pred_labels))\n\n# Confusion Matrix\ncm = confusion_matrix(y_test_labels, y_pred_labels)\nprint(\"\\nConfusion Matrix:\")\nprint(cm)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 14: Save the Model and Components\n","metadata":{}},{"cell_type":"code","source":"# Save model in HDF5 format (as mentioned in paper)\nmodel.save('bangla_sentiment_lstm_model.h5')\n\n# Save model in SavedModel format\nmodel.save('bangla_sentiment_lstm_model')\n\n# Save tokenizer\nwith open('tokenizer.pickle', 'wb') as handle:\n    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n# Save label encoder\nwith open('label_encoder.pickle', 'wb') as handle:\n    pickle.dump(label_encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\nprint(\"Model and components saved successfully!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 15: Function to Predict New Text\n","metadata":{}},{"cell_type":"code","source":"def predict_sentiment(text, model, tokenizer, label_encoder, max_length=100):\n    \"\"\"Predict sentiment for new Bangla text\"\"\"\n    # Clean the text\n    cleaned_text = clean_bangla_text(text)\n    \n    # Convert to sequence\n    sequence = tokenizer.texts_to_sequences([cleaned_text])\n    \n    # Pad sequence\n    padded = pad_sequences(sequence, maxlen=max_length)\n    \n    # Predict\n    prediction = model.predict(padded)\n    predicted_class = np.argmax(prediction, axis=1)[0]\n    \n    # Get confidence scores\n    confidence_scores = prediction[0]\n    \n    # Convert to label\n    predicted_label = label_encoder.inverse_transform([predicted_class])[0]\n    \n    return {\n        'text': text,\n        'cleaned_text': cleaned_text,\n        'predicted_sentiment': predicted_label,\n        'confidence_scores': {\n            label: float(confidence_scores[i]) \n            for i, label in enumerate(label_encoder.classes_)\n        }\n    }\n\n# Test the function\nsample_text = \"আপনার বাংলা টেক্সট এখানে লিখুন\"\nresult = predict_sentiment(sample_text, model, tokenizer, label_encoder)\nprint(result)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}