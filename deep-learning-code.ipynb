{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12250103,"sourceType":"datasetVersion","datasetId":7718676}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Bangla Sentiment Analysis - Complete Solution\n\nThis notebook implements a comprehensive sentiment analysis system for Bangla text, addressing underfitting issues with improved architecture and training strategies.","metadata":{}},{"cell_type":"markdown","source":"## 1. Setup and Imports","metadata":{}},{"cell_type":"code","source":"# Install required packages\n!pip install --upgrade pip setuptools wheel -q\n!pip install torch==2.0.1 --index-url https://download.pytorch.org/whl/cu118 -q\n!pip install transformers==4.30.2 tokenizers==0.13.3 -q\n!pip install scikit-learn pandas numpy matplotlib seaborn tqdm -q\n!pip install nltk -q\n\nimport os\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.auto import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ML libraries\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# Deep Learning\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import Adam, AdamW\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\n# NLP Processing\nimport re\nimport string\nfrom collections import Counter, defaultdict\nimport nltk\n\n# Download NLTK data\nnltk.download('punkt', quiet=True)\nnltk.download('stopwords', quiet=True)\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\n# System utilities\nimport gc\nimport json\nimport random\nfrom datetime import datetime\n\n# Set seeds for reproducibility\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed(42)\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n\n# Create directories\nfor dir_name in ['plots', 'models', 'results', 'embeddings']:\n    os.makedirs(dir_name, exist_ok=True)\n\nprint(\"Setup completed successfully!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Load and Analyze Dataset","metadata":{}},{"cell_type":"code","source":"# Load dataset\ndf = pd.read_csv('/kaggle/input/final-dataset/final-dataset.csv')\nprint(f\"Original dataset shape: {df.shape}\")\n\n# Check unique polarities to determine number of classes\nunique_polarities = df['Polarity'].unique()\nprint(f\"\\nUnique polarities found: {unique_polarities}\")\n\n# Map polarities based on what's in the dataset\nif 'neutral' in unique_polarities:\n    # 3-class case\n    num_classes = 3\n    df['Polarity'] = df['Polarity'].map({'positive': 1, 'negative': 0, 'neutral': 2})\n    class_names = {0: 'Negative', 1: 'Positive', 2: 'Neutral'}\n    print(\"\\nDetected 3-class sentiment analysis\")\nelse:\n    # 2-class case\n    num_classes = 2\n    df['Polarity'] = df['Polarity'].map({'positive': 1, 'negative': 0})\n    class_names = {0: 'Negative', 1: 'Positive'}\n    print(\"\\nDetected 2-class sentiment analysis\")\n\nprint(f\"Number of classes: {num_classes}\")\nprint(\"\\nClass distribution:\")\nprint(df['Polarity'].value_counts().sort_index())\n\n# Add text statistics\ndf['text_length'] = df['Text'].str.len()\ndf['word_count'] = df['Text'].str.split().str.len()\n\nprint(f\"\\nAverage text length: {df['text_length'].mean():.1f} characters\")\nprint(f\"Average word count: {df['word_count'].mean():.1f} words\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Text Preprocessing","metadata":{}},{"cell_type":"code","source":"class BanglaTextPreprocessor:\n    \"\"\"Text preprocessing for Bangla text\"\"\"\n    \n    def __init__(self, min_word_freq=2):\n        self.min_word_freq = min_word_freq\n        self.word2idx = {'<PAD>': 0, '<UNK>': 1, '<SOS>': 2, '<EOS>': 3}\n        self.idx2word = {0: '<PAD>', 1: '<UNK>', 2: '<SOS>', 3: '<EOS>'}\n        self.word_freq = Counter()\n        self.vocab_size = 4\n        \n        # Common Bangla punctuation\n        self.bangla_punct = '।॥,;:!?\\'\"-.…'\n        \n    def clean_text(self, text):\n        \"\"\"Basic cleaning for Bangla text\"\"\"\n        # Convert to string\n        text = str(text)\n        \n        # Remove extra whitespace\n        text = ' '.join(text.split())\n        \n        # Keep Bangla characters, numbers, and basic punctuation\n        # This regex keeps Bangla Unicode range and common characters\n        text = re.sub(r'[^\\u0980-\\u09FF\\s0-9।,!?.-]', ' ', text)\n        \n        # Remove extra spaces\n        text = re.sub(r'\\s+', ' ', text)\n        \n        return text.strip()\n    \n    def tokenize(self, text):\n        \"\"\"Simple word tokenization for Bangla\"\"\"\n        # Clean text first\n        text = self.clean_text(text)\n        \n        # Simple split-based tokenization\n        tokens = text.split()\n        \n        return tokens\n    \n    def build_vocab(self, texts):\n        \"\"\"Build vocabulary from texts\"\"\"\n        # Count word frequencies\n        for text in tqdm(texts, desc=\"Building vocabulary\"):\n            tokens = self.tokenize(text)\n            self.word_freq.update(tokens)\n        \n        # Add words that meet minimum frequency\n        for word, freq in self.word_freq.items():\n            if freq >= self.min_word_freq:\n                if word not in self.word2idx:\n                    self.word2idx[word] = self.vocab_size\n                    self.idx2word[self.vocab_size] = word\n                    self.vocab_size += 1\n        \n        print(f\"Vocabulary size: {self.vocab_size}\")\n        print(f\"Number of unique words: {len(self.word_freq)}\")\n        print(f\"Words meeting min frequency: {self.vocab_size - 4}\")  # Excluding special tokens\n        \n    def text_to_indices(self, text, max_length=None):\n        \"\"\"Convert text to indices\"\"\"\n        tokens = self.tokenize(text)\n        \n        # Convert to indices\n        indices = [self.word2idx.get(token, 1) for token in tokens]  # 1 is <UNK>\n        \n        # Add SOS and EOS\n        indices = [2] + indices + [3]  # 2 is <SOS>, 3 is <EOS>\n        \n        # Truncate or pad if max_length specified\n        if max_length:\n            if len(indices) > max_length:\n                indices = indices[:max_length-1] + [3]  # Keep EOS\n            else:\n                indices = indices + [0] * (max_length - len(indices))  # 0 is <PAD>\n        \n        return indices\n    \n    def texts_to_sequences(self, texts, max_length=None):\n        \"\"\"Convert multiple texts to sequences\"\"\"\n        sequences = []\n        for text in tqdm(texts, desc=\"Converting texts to sequences\"):\n            sequences.append(self.text_to_indices(text, max_length))\n        return sequences\n\n# Initialize preprocessor\npreprocessor = BanglaTextPreprocessor(min_word_freq=2)\n\n# Build vocabulary\nprint(\"Building vocabulary...\")\npreprocessor.build_vocab(df['Text'].values)\n\n# Convert texts to sequences\nmax_seq_length = 100  # Maximum sequence length\nsequences = preprocessor.texts_to_sequences(df['Text'].values, max_length=max_seq_length)\n\nprint(f\"\\nSequence shape: {len(sequences)} x {max_seq_length}\")\nprint(f\"Sample sequence: {sequences[0][:20]}...\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Train Word2Vec Embeddings\n","metadata":{}},{"cell_type":"code","source":"def create_embedding_matrix(vocab_size, embedding_dim=100, method='random'):\n    \"\"\"Create embedding matrix using different methods\"\"\"\n    \n    print(f\"Creating {method} embedding matrix...\")\n    \n    if method == 'random':\n        # Random initialization\n        embedding_matrix = np.random.normal(0, 0.1, (vocab_size, embedding_dim))\n        # Special tokens get different initialization\n        embedding_matrix[0] = np.zeros(embedding_dim)  # PAD token\n        \n    elif method == 'one_hot':\n        # One-hot encoding (not recommended for large vocab)\n        embedding_matrix = np.eye(vocab_size)[:, :embedding_dim]\n        \n    else:  # Default to random\n        embedding_matrix = np.random.normal(0, 0.1, (vocab_size, embedding_dim))\n    \n    print(f\"Embedding matrix shape: {embedding_matrix.shape}\")\n    return embedding_matrix\n\n# Create embedding matrix using random initialization\nembedding_dim = 100\nembedding_matrix = create_embedding_matrix(preprocessor.vocab_size, embedding_dim, method='random')\n\n# Optional: Create a simple frequency-based weighting\nprint(\"\\nApplying frequency-based adjustments...\")\nfor word, idx in preprocessor.word2idx.items():\n    if idx < 4:  # Skip special tokens\n        continue\n    # Adjust embeddings based on frequency (optional)\n    freq = preprocessor.word_freq.get(word, 1)\n    # Words with higher frequency get slightly different initialization\n    if freq > 10:\n        embedding_matrix[idx] *= 1.1\n    elif freq < 3:\n        embedding_matrix[idx] *= 0.9\n\nprint(\"Embedding matrix created successfully!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 5 - Create Dataset Class\n","metadata":{}},{"cell_type":"code","source":"class BanglaDataset(Dataset):\n    \"\"\"Dataset class for Bangla sentiment analysis\"\"\"\n    \n    def __init__(self, sequences, labels):\n        self.sequences = sequences\n        self.labels = labels\n        \n    def __len__(self):\n        return len(self.sequences)\n    \n    def __getitem__(self, idx):\n        return {\n            'sequence': torch.tensor(self.sequences[idx], dtype=torch.long),\n            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n        }\n\ndef create_data_loaders(sequences, labels, batch_size=32, val_split=0.2, test_split=0.1):\n    \"\"\"Create train, validation, and test data loaders\"\"\"\n    \n    # Convert to numpy arrays\n    sequences = np.array(sequences)\n    labels = np.array(labels)\n    \n    # First split: separate test set\n    X_temp, X_test, y_temp, y_test = train_test_split(\n        sequences, labels, test_size=test_split, stratify=labels, random_state=42\n    )\n    \n    # Second split: separate train and validation\n    val_size = val_split / (1 - test_split)\n    X_train, X_val, y_train, y_val = train_test_split(\n        X_temp, y_temp, test_size=val_size, stratify=y_temp, random_state=42\n    )\n    \n    print(f\"Train: {len(X_train)} samples\")\n    print(f\"Val: {len(X_val)} samples\")\n    print(f\"Test: {len(X_test)} samples\")\n    \n    # Create datasets\n    train_dataset = BanglaDataset(X_train, y_train)\n    val_dataset = BanglaDataset(X_val, y_val)\n    test_dataset = BanglaDataset(X_test, y_test)\n    \n    # Create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n    \n    return train_loader, val_loader, test_loader, (X_train, X_val, X_test, y_train, y_val, y_test)\n\n# Create data loaders\ntrain_loader, val_loader, test_loader, data_splits = create_data_loaders(\n    sequences, \n    df['Polarity'].values,\n    batch_size=64,\n    val_split=0.2,\n    test_split=0.1\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 6 - CNN Model","metadata":{}},{"cell_type":"code","source":"class CNNSentimentClassifier(nn.Module):\n    \"\"\"CNN model for sentiment classification\"\"\"\n    \n    def __init__(self, vocab_size, embedding_dim, num_classes, \n                 embedding_matrix=None, num_filters=100, filter_sizes=[3, 4, 5], \n                 dropout=0.5, freeze_embeddings=False):\n        super(CNNSentimentClassifier, self).__init__()\n        \n        # Embedding layer\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        \n        # Initialize with pre-trained embeddings if provided\n        if embedding_matrix is not None:\n            self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix).float())\n            # Optionally freeze embeddings\n            if freeze_embeddings:\n                self.embedding.weight.requires_grad = False\n        \n        # Convolutional layers\n        self.convs = nn.ModuleList([\n            nn.Conv1d(embedding_dim, num_filters, kernel_size=k)\n            for k in filter_sizes\n        ])\n        \n        # Batch normalization\n        self.batch_norms = nn.ModuleList([\n            nn.BatchNorm1d(num_filters)\n            for _ in filter_sizes\n        ])\n        \n        # Dropout\n        self.dropout = nn.Dropout(dropout)\n        \n        # Fully connected layers\n        fc_input_dim = len(filter_sizes) * num_filters\n        self.fc1 = nn.Linear(fc_input_dim, 128)\n        self.batch_norm_fc = nn.BatchNorm1d(128)\n        self.fc2 = nn.Linear(128, num_classes)\n        \n        # Activation\n        self.relu = nn.ReLU()\n        \n    def forward(self, x):\n        # x shape: (batch_size, sequence_length)\n        \n        # Embedding\n        x = self.embedding(x)  # (batch_size, seq_len, embedding_dim)\n        x = x.permute(0, 2, 1)  # (batch_size, embedding_dim, seq_len)\n        \n        # Apply convolutions\n        conv_outputs = []\n        for conv, bn in zip(self.convs, self.batch_norms):\n            conv_out = conv(x)  # (batch_size, num_filters, new_seq_len)\n            conv_out = bn(conv_out)  # Batch normalization\n            conv_out = self.relu(conv_out)\n            # Max pooling over time\n            pooled = F.max_pool1d(conv_out, conv_out.size(2))  # (batch_size, num_filters, 1)\n            pooled = pooled.squeeze(2)  # (batch_size, num_filters)\n            conv_outputs.append(pooled)\n        \n        # Concatenate all conv outputs\n        x = torch.cat(conv_outputs, dim=1)  # (batch_size, len(filter_sizes) * num_filters)\n        \n        # Apply dropout\n        x = self.dropout(x)\n        \n        # Fully connected layers\n        x = self.fc1(x)\n        x = self.batch_norm_fc(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        \n        return x\n\n# Initialize CNN model\ncnn_model = CNNSentimentClassifier(\n    vocab_size=preprocessor.vocab_size,\n    embedding_dim=100,\n    num_classes=num_classes,\n    embedding_matrix=embedding_matrix,\n    num_filters=100,\n    filter_sizes=[3, 4, 5],\n    dropout=0.5,\n    freeze_embeddings=False\n).to(device)\n\nprint(f\"CNN Model initialized\")\nprint(f\"Total parameters: {sum(p.numel() for p in cnn_model.parameters()):,}\")\nprint(f\"Trainable parameters: {sum(p.numel() for p in cnn_model.parameters() if p.requires_grad):,}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 7 - BiLSTM Model\n","metadata":{}},{"cell_type":"code","source":"class BiLSTMSentimentClassifier(nn.Module):\n    \"\"\"Bidirectional LSTM model for sentiment classification\"\"\"\n    \n    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, \n                 num_classes, embedding_matrix=None, dropout=0.5, freeze_embeddings=False):\n        super(BiLSTMSentimentClassifier, self).__init__()\n        \n        # Embedding layer\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        \n        # Initialize with pre-trained embeddings if provided\n        if embedding_matrix is not None:\n            self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix).float())\n            if freeze_embeddings:\n                self.embedding.weight.requires_grad = False\n        \n        # BiLSTM layers\n        self.lstm = nn.LSTM(\n            embedding_dim,\n            hidden_dim,\n            num_layers,\n            batch_first=True,\n            bidirectional=True,\n            dropout=dropout if num_layers > 1 else 0\n        )\n        \n        # Attention mechanism\n        self.attention = nn.Linear(hidden_dim * 2, 1)\n        \n        # Dropout\n        self.dropout = nn.Dropout(dropout)\n        \n        # Fully connected layers\n        # *2 for bidirectional\n        self.fc1 = nn.Linear(hidden_dim * 2, 128)\n        self.batch_norm = nn.BatchNorm1d(128)\n        self.fc2 = nn.Linear(128, num_classes)\n        \n        # Activation\n        self.relu = nn.ReLU()\n        self.tanh = nn.Tanh()\n        \n    def attention_weights(self, lstm_output):\n        \"\"\"Calculate attention weights\"\"\"\n        # lstm_output shape: (batch_size, seq_len, hidden_dim * 2)\n        attention_scores = self.attention(lstm_output)  # (batch_size, seq_len, 1)\n        attention_scores = attention_scores.squeeze(-1)  # (batch_size, seq_len)\n        attention_weights = F.softmax(attention_scores, dim=1)  # (batch_size, seq_len)\n        return attention_weights\n        \n    def forward(self, x):\n        # x shape: (batch_size, sequence_length)\n        \n        # Get actual lengths (before padding)\n        lengths = (x != 0).sum(dim=1).cpu()\n        \n        # Embedding\n        x = self.embedding(x)  # (batch_size, seq_len, embedding_dim)\n        \n        # Pack padded sequence\n        packed = nn.utils.rnn.pack_padded_sequence(\n            x, lengths, batch_first=True, enforce_sorted=False\n        )\n        \n        # LSTM\n        packed_output, (hidden, cell) = self.lstm(packed)\n        \n        # Unpack sequence\n        output, output_lengths = nn.utils.rnn.pad_packed_sequence(\n            packed_output, batch_first=True\n        )\n        # output shape: (batch_size, seq_len, hidden_dim * 2)\n        \n        # Apply attention\n        attention_weights = self.attention_weights(output)\n        # attention_weights shape: (batch_size, seq_len)\n        \n        # Apply attention to get weighted representation\n        attended = torch.bmm(\n            attention_weights.unsqueeze(1), \n            output\n        ).squeeze(1)\n        # attended shape: (batch_size, hidden_dim * 2)\n        \n        # Apply dropout\n        attended = self.dropout(attended)\n        \n        # Fully connected layers\n        x = self.fc1(attended)\n        x = self.batch_norm(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        \n        return x\n\n# Initialize BiLSTM model\nbilstm_model = BiLSTMSentimentClassifier(\n    vocab_size=preprocessor.vocab_size,\n    embedding_dim=100,\n    hidden_dim=128,\n    num_layers=2,\n    num_classes=num_classes,\n    embedding_matrix=embedding_matrix,\n    dropout=0.5,\n    freeze_embeddings=False\n).to(device)\n\nprint(f\"BiLSTM Model initialized with Attention\")\nprint(f\"Total parameters: {sum(p.numel() for p in bilstm_model.parameters()):,}\")\nprint(f\"Trainable parameters: {sum(p.numel() for p in bilstm_model.parameters() if p.requires_grad):,}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 7a - GRU Model\n","metadata":{}},{"cell_type":"code","source":"class GRUSentimentClassifier(nn.Module):\n    \"\"\"GRU model for sentiment classification\"\"\"\n    \n    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, \n                 num_classes, embedding_matrix=None, dropout=0.5, freeze_embeddings=False):\n        super(GRUSentimentClassifier, self).__init__()\n        \n        # Embedding layer\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        \n        # Initialize with pre-trained embeddings if provided\n        if embedding_matrix is not None:\n            self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix).float())\n            if freeze_embeddings:\n                self.embedding.weight.requires_grad = False\n        \n        # GRU layers\n        self.gru = nn.GRU(\n            embedding_dim,\n            hidden_dim,\n            num_layers,\n            batch_first=True,\n            bidirectional=True,\n            dropout=dropout if num_layers > 1 else 0\n        )\n        \n        # Attention mechanism\n        self.attention = nn.Linear(hidden_dim * 2, 1)\n        \n        # Dropout\n        self.dropout = nn.Dropout(dropout)\n        \n        # Fully connected layers\n        # *2 for bidirectional\n        self.fc1 = nn.Linear(hidden_dim * 2, 128)\n        self.batch_norm = nn.BatchNorm1d(128)\n        self.fc2 = nn.Linear(128, num_classes)\n        \n        # Activation\n        self.relu = nn.ReLU()\n        \n    def attention_weights(self, gru_output):\n        \"\"\"Calculate attention weights\"\"\"\n        # gru_output shape: (batch_size, seq_len, hidden_dim * 2)\n        attention_scores = self.attention(gru_output)  # (batch_size, seq_len, 1)\n        attention_scores = attention_scores.squeeze(-1)  # (batch_size, seq_len)\n        attention_weights = F.softmax(attention_scores, dim=1)  # (batch_size, seq_len)\n        return attention_weights\n        \n    def forward(self, x):\n        # x shape: (batch_size, sequence_length)\n        \n        # Get actual lengths (before padding)\n        lengths = (x != 0).sum(dim=1).cpu()\n        \n        # Embedding\n        x = self.embedding(x)  # (batch_size, seq_len, embedding_dim)\n        \n        # Pack padded sequence\n        packed = nn.utils.rnn.pack_padded_sequence(\n            x, lengths, batch_first=True, enforce_sorted=False\n        )\n        \n        # GRU\n        packed_output, hidden = self.gru(packed)\n        \n        # Unpack sequence\n        output, output_lengths = nn.utils.rnn.pad_packed_sequence(\n            packed_output, batch_first=True\n        )\n        # output shape: (batch_size, seq_len, hidden_dim * 2)\n        \n        # Apply attention\n        attention_weights = self.attention_weights(output)\n        \n        # Apply attention to get weighted representation\n        attended = torch.bmm(\n            attention_weights.unsqueeze(1), \n            output\n        ).squeeze(1)\n        # attended shape: (batch_size, hidden_dim * 2)\n        \n        # Apply dropout\n        attended = self.dropout(attended)\n        \n        # Fully connected layers\n        x = self.fc1(attended)\n        x = self.batch_norm(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        \n        return x\n\n# Initialize GRU model\ngru_model = GRUSentimentClassifier(\n    vocab_size=preprocessor.vocab_size,\n    embedding_dim=100,\n    hidden_dim=128,\n    num_layers=2,\n    num_classes=num_classes,\n    embedding_matrix=embedding_matrix,\n    dropout=0.5,\n    freeze_embeddings=False\n).to(device)\n\nprint(f\"GRU Model initialized with Attention\")\nprint(f\"Total parameters: {sum(p.numel() for p in gru_model.parameters()):,}\")\nprint(f\"Trainable parameters: {sum(p.numel() for p in gru_model.parameters() if p.requires_grad):,}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 7b - Hybrid CNN-LSTM Model\n","metadata":{}},{"cell_type":"code","source":"class CNNLSTMSentimentClassifier(nn.Module):\n    \"\"\"Hybrid CNN-LSTM model for sentiment classification\"\"\"\n    \n    def __init__(self, vocab_size, embedding_dim, num_filters, filter_sizes,\n                 lstm_hidden_dim, num_lstm_layers, num_classes, \n                 embedding_matrix=None, dropout=0.5, freeze_embeddings=False):\n        super(CNNLSTMSentimentClassifier, self).__init__()\n        \n        # Embedding layer\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        \n        # Initialize with pre-trained embeddings if provided\n        if embedding_matrix is not None:\n            self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix).float())\n            if freeze_embeddings:\n                self.embedding.weight.requires_grad = False\n        \n        # CNN layers\n        self.convs = nn.ModuleList([\n            nn.Conv1d(embedding_dim, num_filters, kernel_size=k)\n            for k in filter_sizes\n        ])\n        \n        # Batch normalization for CNN\n        self.batch_norms_cnn = nn.ModuleList([\n            nn.BatchNorm1d(num_filters)\n            for _ in filter_sizes\n        ])\n        \n        # LSTM layer\n        # Input size is total number of filters from all conv layers\n        lstm_input_size = len(filter_sizes) * num_filters\n        self.lstm = nn.LSTM(\n            lstm_input_size,\n            lstm_hidden_dim,\n            num_lstm_layers,\n            batch_first=True,\n            bidirectional=True,\n            dropout=dropout if num_lstm_layers > 1 else 0\n        )\n        \n        # Attention for LSTM output\n        self.attention = nn.Linear(lstm_hidden_dim * 2, 1)\n        \n        # Dropout\n        self.dropout = nn.Dropout(dropout)\n        \n        # Fully connected layers\n        self.fc1 = nn.Linear(lstm_hidden_dim * 2, 128)\n        self.batch_norm_fc = nn.BatchNorm1d(128)\n        self.fc2 = nn.Linear(128, num_classes)\n        \n        # Activation\n        self.relu = nn.ReLU()\n        \n    def forward(self, x):\n        # x shape: (batch_size, sequence_length)\n        batch_size = x.size(0)\n        seq_len = x.size(1)\n        \n        # Embedding\n        x_embed = self.embedding(x)  # (batch_size, seq_len, embedding_dim)\n        x_cnn = x_embed.permute(0, 2, 1)  # (batch_size, embedding_dim, seq_len)\n        \n        # Apply CNNs\n        conv_outputs = []\n        for conv, bn in zip(self.convs, self.batch_norms_cnn):\n            conv_out = conv(x_cnn)  # (batch_size, num_filters, new_seq_len)\n            conv_out = bn(conv_out)\n            conv_out = self.relu(conv_out)\n            conv_outputs.append(conv_out)\n        \n        # Concatenate CNN outputs along filter dimension\n        # Each conv_out has shape (batch_size, num_filters, conv_seq_len)\n        # We need to transpose to (batch_size, conv_seq_len, num_filters)\n        cnn_features = []\n        min_seq_len = min(conv_out.size(2) for conv_out in conv_outputs)\n        \n        for conv_out in conv_outputs:\n            # Trim to minimum sequence length for concatenation\n            conv_out = conv_out[:, :, :min_seq_len]\n            conv_out = conv_out.permute(0, 2, 1)  # (batch_size, min_seq_len, num_filters)\n            cnn_features.append(conv_out)\n        \n        # Concatenate all CNN features\n        cnn_combined = torch.cat(cnn_features, dim=2)  # (batch_size, min_seq_len, total_filters)\n        \n        # Pass through LSTM\n        lstm_out, (hidden, cell) = self.lstm(cnn_combined)\n        # lstm_out shape: (batch_size, min_seq_len, hidden_dim * 2)\n        \n        # Apply attention\n        attention_scores = self.attention(lstm_out)  # (batch_size, min_seq_len, 1)\n        attention_scores = attention_scores.squeeze(-1)  # (batch_size, min_seq_len)\n        attention_weights = F.softmax(attention_scores, dim=1)  # (batch_size, min_seq_len)\n        \n        # Apply attention to get weighted representation\n        attended = torch.bmm(\n            attention_weights.unsqueeze(1), \n            lstm_out\n        ).squeeze(1)\n        # attended shape: (batch_size, hidden_dim * 2)\n        \n        # Apply dropout\n        attended = self.dropout(attended)\n        \n        # Fully connected layers\n        x = self.fc1(attended)\n        x = self.batch_norm_fc(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        \n        return x\n\n# Initialize CNN-LSTM model\ncnn_lstm_model = CNNLSTMSentimentClassifier(\n    vocab_size=preprocessor.vocab_size,\n    embedding_dim=100,\n    num_filters=100,\n    filter_sizes=[3, 4, 5],\n    lstm_hidden_dim=128,\n    num_lstm_layers=2,\n    num_classes=num_classes,\n    embedding_matrix=embedding_matrix,\n    dropout=0.5,\n    freeze_embeddings=False\n).to(device)\n\nprint(f\"CNN-LSTM Hybrid Model initialized\")\nprint(f\"Total parameters: {sum(p.numel() for p in cnn_lstm_model.parameters()):,}\")\nprint(f\"Trainable parameters: {sum(p.numel() for p in cnn_lstm_model.parameters() if p.requires_grad):,}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 8 - Training Functions\n","metadata":{}},{"cell_type":"code","source":"def train_epoch(model, train_loader, criterion, optimizer, device):\n    \"\"\"Train for one epoch\"\"\"\n    model.train()\n    total_loss = 0\n    correct = 0\n    total = 0\n    \n    for batch in tqdm(train_loader, desc=\"Training\"):\n        sequences = batch['sequence'].to(device)\n        labels = batch['label'].to(device)\n        \n        # Zero gradients\n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = model(sequences)\n        loss = criterion(outputs, labels)\n        \n        # Backward pass\n        loss.backward()\n        \n        # Gradient clipping\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        \n        optimizer.step()\n        \n        # Statistics\n        total_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n    \n    avg_loss = total_loss / len(train_loader)\n    accuracy = correct / total\n    \n    return avg_loss, accuracy\n\ndef evaluate(model, data_loader, criterion, device):\n    \"\"\"Evaluate model\"\"\"\n    model.eval()\n    total_loss = 0\n    correct = 0\n    total = 0\n    all_predictions = []\n    all_labels = []\n    all_probs = []\n    \n    with torch.no_grad():\n        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n            sequences = batch['sequence'].to(device)\n            labels = batch['label'].to(device)\n            \n            # Forward pass\n            outputs = model(sequences)\n            loss = criterion(outputs, labels)\n            \n            # Get probabilities\n            probs = F.softmax(outputs, dim=1)\n            \n            # Statistics\n            total_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            \n            all_predictions.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n            all_probs.extend(probs.cpu().numpy())\n    \n    avg_loss = total_loss / len(data_loader)\n    accuracy = correct / total\n    f1 = f1_score(all_labels, all_predictions, average='weighted')\n    \n    return avg_loss, accuracy, f1, all_predictions, all_labels, all_probs\n\ndef train_model(model, train_loader, val_loader, num_epochs=20, learning_rate=0.001, model_name=\"model\"):\n    \"\"\"Complete training loop\"\"\"\n    \n    # Loss and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5, verbose=True)\n    \n    # Training history\n    history = {\n        'train_loss': [], 'train_acc': [],\n        'val_loss': [], 'val_acc': [], 'val_f1': []\n    }\n    \n    best_val_f1 = 0\n    best_model_state = None\n    patience_counter = 0\n    early_stopping_patience = 5\n    \n    print(f\"\\nTraining {model_name}...\")\n    print(\"=\"*70)\n    \n    for epoch in range(num_epochs):\n        # Train\n        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n        \n        # Validate\n        val_loss, val_acc, val_f1, _, _, _ = evaluate(model, val_loader, criterion, device)\n        \n        # Update scheduler\n        scheduler.step(val_loss)\n        \n        # Save history\n        history['train_loss'].append(train_loss)\n        history['train_acc'].append(train_acc)\n        history['val_loss'].append(val_loss)\n        history['val_acc'].append(val_acc)\n        history['val_f1'].append(val_f1)\n        \n        # Save best model\n        if val_f1 > best_val_f1:\n            best_val_f1 = val_f1\n            best_model_state = model.state_dict().copy()\n            patience_counter = 0\n        else:\n            patience_counter += 1\n        \n        # Print progress\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n        \n        if patience_counter == 0:\n            print(\"✓ New best model!\")\n        \n        print(\"-\"*50)\n        \n        # Early stopping\n        if patience_counter >= early_stopping_patience:\n            print(f\"Early stopping triggered after {epoch+1} epochs\")\n            break\n    \n    # Load best model\n    if best_model_state is not None:\n        model.load_state_dict(best_model_state)\n        print(f\"\\nLoaded best model with Val F1: {best_val_f1:.4f}\")\n    \n    return model, history","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 9 - Train CNN Model\n","metadata":{}},{"cell_type":"code","source":"# Train CNN model\ncnn_model, cnn_history = train_model(\n    cnn_model,\n    train_loader,\n    val_loader,\n    num_epochs=25,\n    learning_rate=0.001,\n    model_name=\"CNN\"\n)\n\n# Evaluate on test set\nprint(\"\\nEvaluating CNN on test set...\")\ncriterion = nn.CrossEntropyLoss()\ntest_loss, test_acc, test_f1, test_preds, test_labels, test_probs = evaluate(\n    cnn_model, test_loader, criterion, device\n)\n\nprint(f\"\\nCNN Test Results:\")\nprint(f\"Test Loss: {test_loss:.4f}\")\nprint(f\"Test Accuracy: {test_acc:.4f}\")\nprint(f\"Test F1 Score: {test_f1:.4f}\")\n\n# Save CNN results\ncnn_results = {\n    'history': cnn_history,\n    'test_metrics': {\n        'loss': test_loss,\n        'accuracy': test_acc,\n        'f1': test_f1\n    },\n    'predictions': test_preds,\n    'labels': test_labels,\n    'probabilities': test_probs\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 10 - Train BiLSTM Model","metadata":{}},{"cell_type":"code","source":"# Train BiLSTM model\nbilstm_model, bilstm_history = train_model(\n    bilstm_model,\n    train_loader,\n    val_loader,\n    num_epochs=25,\n    learning_rate=0.001,\n    model_name=\"BiLSTM\"\n)\n\n# Evaluate on test set\nprint(\"\\nEvaluating BiLSTM on test set...\")\ntest_loss, test_acc, test_f1, test_preds, test_labels, test_probs = evaluate(\n    bilstm_model, test_loader, criterion, device\n)\n\nprint(f\"\\nBiLSTM Test Results:\")\nprint(f\"Test Loss: {test_loss:.4f}\")\nprint(f\"Test Accuracy: {test_acc:.4f}\")\nprint(f\"Test F1 Score: {test_f1:.4f}\")\n\n# Save BiLSTM results\nbilstm_results = {\n    'history': bilstm_history,\n    'test_metrics': {\n        'loss': test_loss,\n        'accuracy': test_acc,\n        'f1': test_f1\n    },\n    'predictions': test_preds,\n    'labels': test_labels,\n    'probabilities': test_probs\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 10a - Train GRU Model\n","metadata":{}},{"cell_type":"code","source":"# Train GRU model\ngru_model, gru_history = train_model(\n    gru_model,\n    train_loader,\n    val_loader,\n    num_epochs=25,\n    learning_rate=0.001,\n    model_name=\"GRU\"\n)\n\n# Evaluate on test set\nprint(\"\\nEvaluating GRU on test set...\")\ncriterion = nn.CrossEntropyLoss()\ntest_loss, test_acc, test_f1, test_preds, test_labels, test_probs = evaluate(\n    gru_model, test_loader, criterion, device\n)\n\nprint(f\"\\nGRU Test Results:\")\nprint(f\"Test Loss: {test_loss:.4f}\")\nprint(f\"Test Accuracy: {test_acc:.4f}\")\nprint(f\"Test F1 Score: {test_f1:.4f}\")\n\n# Save GRU results\ngru_results = {\n    'history': gru_history,\n    'test_metrics': {\n        'loss': test_loss,\n        'accuracy': test_acc,\n        'f1': test_f1\n    },\n    'predictions': test_preds,\n    'labels': test_labels,\n    'probabilities': test_probs\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 10b - Train CNN-LSTM Model\n","metadata":{}},{"cell_type":"code","source":"# Train CNN-LSTM model\ncnn_lstm_model, cnn_lstm_history = train_model(\n    cnn_lstm_model,\n    train_loader,\n    val_loader,\n    num_epochs=25,\n    learning_rate=0.001,\n    model_name=\"CNN-LSTM\"\n)\n\n# Evaluate on test set\nprint(\"\\nEvaluating CNN-LSTM on test set...\")\ntest_loss, test_acc, test_f1, test_preds, test_labels, test_probs = evaluate(\n    cnn_lstm_model, test_loader, criterion, device\n)\n\nprint(f\"\\nCNN-LSTM Test Results:\")\nprint(f\"Test Loss: {test_loss:.4f}\")\nprint(f\"Test Accuracy: {test_acc:.4f}\")\nprint(f\"Test F1 Score: {test_f1:.4f}\")\n\n# Save CNN-LSTM results\ncnn_lstm_results = {\n    'history': cnn_lstm_history,\n    'test_metrics': {\n        'loss': test_loss,\n        'accuracy': test_acc,\n        'f1': test_f1\n    },\n    'predictions': test_preds,\n    'labels': test_labels,\n    'probabilities': test_probs\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 11 - Visualize Training History\n","metadata":{}},{"cell_type":"code","source":"def plot_training_history(histories, model_names):\n    \"\"\"Plot training history for multiple models\"\"\"\n    \n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n    \n    # Colors for different models\n    colors = ['blue', 'red', 'green', 'orange']\n    \n    # Plot training loss\n    for i, (history, name) in enumerate(zip(histories, model_names)):\n        epochs = range(1, len(history['train_loss']) + 1)\n        axes[0, 0].plot(epochs, history['train_loss'], \n                       color=colors[i], linestyle='-', label=f'{name} Train', alpha=0.8)\n        axes[0, 0].plot(epochs, history['val_loss'], \n                       color=colors[i], linestyle='--', label=f'{name} Val', alpha=0.8)\n    \n    axes[0, 0].set_xlabel('Epochs')\n    axes[0, 0].set_ylabel('Loss')\n    axes[0, 0].set_title('Training and Validation Loss')\n    axes[0, 0].legend(loc='upper right', fontsize='small')\n    axes[0, 0].grid(True, alpha=0.3)\n    \n    # Plot training accuracy\n    for i, (history, name) in enumerate(zip(histories, model_names)):\n        epochs = range(1, len(history['train_acc']) + 1)\n        axes[0, 1].plot(epochs, history['train_acc'], \n                       color=colors[i], linestyle='-', label=f'{name} Train', alpha=0.8)\n        axes[0, 1].plot(epochs, history['val_acc'], \n                       color=colors[i], linestyle='--', label=f'{name} Val', alpha=0.8)\n    \n    axes[0, 1].set_xlabel('Epochs')\n    axes[0, 1].set_ylabel('Accuracy')\n    axes[0, 1].set_title('Training and Validation Accuracy')\n    axes[0, 1].legend(loc='lower right', fontsize='small')\n    axes[0, 1].grid(True, alpha=0.3)\n    \n    # Plot validation F1 score\n    for i, (history, name) in enumerate(zip(histories, model_names)):\n        epochs = range(1, len(history['val_f1']) + 1)\n        axes[1, 0].plot(epochs, history['val_f1'], \n                       color=colors[i], marker='o', label=name, markersize=4, alpha=0.8)\n    \n    axes[1, 0].set_xlabel('Epochs')\n    axes[1, 0].set_ylabel('F1 Score')\n    axes[1, 0].set_title('Validation F1 Score')\n    axes[1, 0].legend(loc='lower right')\n    axes[1, 0].grid(True, alpha=0.3)\n    \n    # Plot final metrics comparison\n    model_names_list = list(model_names)\n    metrics = ['Accuracy', 'F1 Score']\n    x = np.arange(len(model_names_list))\n    width = 0.35\n    \n    # Get final validation metrics\n    val_accs = [hist['val_acc'][-1] for hist in histories]\n    val_f1s = [hist['val_f1'][-1] for hist in histories]\n    \n    bars1 = axes[1, 1].bar(x - width/2, val_accs, width, label='Accuracy', color='skyblue')\n    bars2 = axes[1, 1].bar(x + width/2, val_f1s, width, label='F1 Score', color='lightcoral')\n    \n    axes[1, 1].set_xlabel('Model')\n    axes[1, 1].set_ylabel('Score')\n    axes[1, 1].set_title('Final Validation Metrics Comparison')\n    axes[1, 1].set_xticks(x)\n    axes[1, 1].set_xticklabels(model_names_list, rotation=45, ha='right')\n    axes[1, 1].legend()\n    axes[1, 1].grid(True, alpha=0.3, axis='y')\n    \n    # Add value labels on bars\n    for i, (acc, f1) in enumerate(zip(val_accs, val_f1s)):\n        axes[1, 1].text(i - width/2, acc + 0.01, f'{acc:.3f}', \n                       ha='center', va='bottom', fontsize=8)\n        axes[1, 1].text(i + width/2, f1 + 0.01, f'{f1:.3f}', \n                       ha='center', va='bottom', fontsize=8)\n    \n    plt.tight_layout()\n    plt.savefig('plots/dl_models_training_history_all.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\n# Plot training histories for all models\nplot_training_history(\n    [cnn_history, bilstm_history, gru_history, cnn_lstm_history],\n    ['CNN', 'BiLSTM', 'GRU', 'CNN-LSTM']\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 12 - Compare Model Performance\n","metadata":{}},{"cell_type":"code","source":"def compare_model_performance(results_dict, model_names, num_classes, class_names):\n    \"\"\"Compare performance of multiple models\"\"\"\n    \n    # Create figure with appropriate size for 4 models\n    fig = plt.figure(figsize=(20, 12))\n    \n    # 1. Test Metrics Comparison (Top left)\n    ax1 = plt.subplot(2, 3, 1)\n    metrics = ['Loss', 'Accuracy', 'F1 Score']\n    model_list = list(model_names)\n    x = np.arange(len(model_list))\n    width = 0.25\n    \n    # Extract metrics for each model\n    for i, metric in enumerate(metrics):\n        values = []\n        for model in model_list:\n            if metric == 'Loss':\n                values.append(results_dict[model]['test_metrics']['loss'])\n            elif metric == 'Accuracy':\n                values.append(results_dict[model]['test_metrics']['accuracy'])\n            else:  # F1 Score\n                values.append(results_dict[model]['test_metrics']['f1'])\n        \n        offset = (i - 1) * width\n        bars = ax1.bar(x + offset, values, width, label=metric)\n        \n        # Add value labels\n        for j, v in enumerate(values):\n            ax1.text(j + offset, v + 0.01, f'{v:.3f}', \n                    ha='center', va='bottom', fontsize=8)\n    \n    ax1.set_xlabel('Model')\n    ax1.set_ylabel('Score')\n    ax1.set_title('Test Metrics Comparison')\n    ax1.set_xticks(x)\n    ax1.set_xticklabels(model_list, rotation=45, ha='right')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3, axis='y')\n    \n    # 2-5. Confusion Matrices for each model\n    positions = [(0, 1), (0, 2), (1, 0), (1, 1)]\n    for idx, (model_name, pos) in enumerate(zip(model_list, positions)):\n        ax = plt.subplot(2, 3, pos[0] * 3 + pos[1] + 1)\n        cm = confusion_matrix(results_dict[model_name]['labels'], \n                             results_dict[model_name]['predictions'])\n        labels = [class_names[i] for i in range(num_classes)]\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n                    xticklabels=labels, yticklabels=labels)\n        ax.set_title(f'{model_name} Confusion Matrix')\n        ax.set_ylabel('True Label')\n        ax.set_xlabel('Predicted Label')\n    \n    # 6. Per-class F1 Scores Comparison (Bottom right)\n    ax6 = plt.subplot(2, 3, 6)\n    x_classes = np.arange(num_classes)\n    width = 0.2\n    \n    for i, model_name in enumerate(model_list):\n        f1_per_class = f1_score(results_dict[model_name]['labels'], \n                               results_dict[model_name]['predictions'], \n                               average=None)\n        offset = (i - 1.5) * width\n        bars = ax6.bar(x_classes + offset, f1_per_class, width, label=model_name)\n        \n        # Add value labels\n        for j, v in enumerate(f1_per_class):\n            ax6.text(j + offset, v + 0.01, f'{v:.2f}', \n                    ha='center', va='bottom', fontsize=7, rotation=90)\n    \n    ax6.set_xlabel('Class')\n    ax6.set_ylabel('F1 Score')\n    ax6.set_title('Per-class F1 Scores Comparison')\n    ax6.set_xticks(x_classes)\n    ax6.set_xticklabels([class_names[i] for i in range(num_classes)])\n    ax6.legend()\n    ax6.grid(True, alpha=0.3, axis='y')\n    \n    plt.tight_layout()\n    plt.savefig('plots/all_models_comparison.png', dpi=300, bbox_inches='tight')\n    plt.show()\n    \n    # Print comprehensive summary\n    print(\"\\nCOMPREHENSIVE MODEL COMPARISON SUMMARY\")\n    print(\"=\"*80)\n    print(f\"{'Model':<12} {'Accuracy':<10} {'F1 Score':<10} {'Loss':<10} {'Parameters':<15}\")\n    print(\"-\"*80)\n    \n    for model_name in model_list:\n        metrics = results_dict[model_name]['test_metrics']\n        # Get parameter count (you'll need to pass models_dict to this function)\n        print(f\"{model_name:<12} {metrics['accuracy']:<10.4f} {metrics['f1']:<10.4f} {metrics['loss']:<10.4f}\")\n    \n    print(\"\\nPer-class F1 Scores:\")\n    print(\"-\"*80)\n    print(f\"{'Model':<12}\", end='')\n    for i in range(num_classes):\n        print(f\"{class_names[i]:<15}\", end='')\n    print()\n    print(\"-\"*80)\n    \n    for model_name in model_list:\n        f1_per_class = f1_score(results_dict[model_name]['labels'], \n                               results_dict[model_name]['predictions'], \n                               average=None)\n        print(f\"{model_name:<12}\", end='')\n        for f1 in f1_per_class:\n            print(f\"{f1:<15.4f}\", end='')\n        print()\n\n# Compare all model performance\nresults_dict = {\n    'CNN': cnn_results,\n    'BiLSTM': bilstm_results,\n    'GRU': gru_results,\n    'CNN-LSTM': cnn_lstm_results\n}\n\ncompare_model_performance(results_dict, ['CNN', 'BiLSTM', 'GRU', 'CNN-LSTM'], \n                         num_classes, class_names)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 13 - Classification Reports\n","metadata":{}},{"cell_type":"code","source":"# Print detailed classification reports for all models\nprint(\"=\"*80)\nprint(\"DETAILED CLASSIFICATION REPORTS - ALL MODELS\")\nprint(\"=\"*80)\n\nmodels_to_report = [\n    ('CNN', cnn_results),\n    ('BiLSTM', bilstm_results),\n    ('GRU', gru_results),\n    ('CNN-LSTM', cnn_lstm_results)\n]\n\nfor model_name, results in models_to_report:\n    print(f\"\\n{model_name} Model:\")\n    print(\"-\"*60)\n    print(classification_report(\n        results['labels'], \n        results['predictions'],\n        target_names=[class_names[i] for i in range(num_classes)],\n        digits=4\n    ))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 14 - Error Analysis\n","metadata":{}},{"cell_type":"code","source":"def analyze_errors(model, test_loader, model_name, num_samples=10):\n    \"\"\"Analyze model errors\"\"\"\n    \n    model.eval()\n    errors = []\n    \n    with torch.no_grad():\n        for batch in test_loader:\n            sequences = batch['sequence'].to(device)\n            labels = batch['label'].to(device)\n            \n            outputs = model(sequences)\n            probs = F.softmax(outputs, dim=1)\n            _, predicted = torch.max(outputs.data, 1)\n            \n            # Find errors\n            error_mask = predicted != labels\n            if error_mask.any():\n                error_indices = error_mask.nonzero(as_tuple=True)[0]\n                \n                for idx in error_indices:\n                    # Convert sequence back to text\n                    seq = sequences[idx].cpu().numpy()\n                    tokens = [preprocessor.idx2word.get(idx, '<UNK>') \n                             for idx in seq if idx not in [0, 2, 3]]  # Exclude special tokens\n                    text = ' '.join(tokens)\n                    \n                    errors.append({\n                        'text': text,\n                        'true_label': class_names[labels[idx].item()],\n                        'predicted_label': class_names[predicted[idx].item()],\n                        'confidence': probs[idx].max().item(),\n                        'probs': probs[idx].cpu().numpy()\n                    })\n    \n    print(f\"\\n{model_name} - Error Analysis\")\n    print(\"=\"*70)\n    print(f\"Total errors: {len(errors)}\")\n    print(f\"\\nSample misclassified texts:\")\n    print(\"-\"*70)\n    \n    # Sort by confidence (high confidence errors first)\n    errors_sorted = sorted(errors, key=lambda x: x['confidence'], reverse=True)\n    \n    for i, error in enumerate(errors_sorted[:num_samples]):\n        print(f\"\\nError {i+1}:\")\n        print(f\"Text: {error['text'][:100]}...\")\n        print(f\"True label: {error['true_label']}\")\n        print(f\"Predicted: {error['predicted_label']}\")\n        print(f\"Confidence: {error['confidence']:.4f}\")\n        \n        # Show probabilities for all classes\n        prob_str = \", \".join([f\"{class_names[j]}: {error['probs'][j]:.3f}\" \n                             for j in range(num_classes)])\n        print(f\"All probabilities: {prob_str}\")\n\n# Analyze errors for both models\nanalyze_errors(cnn_model, test_loader, \"CNN\", num_samples=5)\nanalyze_errors(bilstm_model, test_loader, \"BiLSTM\", num_samples=5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 15 - Save Models and Results\n","metadata":{}},{"cell_type":"code","source":"# Save all models and results\ndef save_all_dl_models(models_dict, results_dict, preprocessor):\n    \"\"\"Save all deep learning models and results\"\"\"\n    \n    # Save models\n    for model_name, model in models_dict.items():\n        model_path = f'models/{model_name.lower()}_sentiment_model.pth'\n        torch.save({\n            'model_state_dict': model.state_dict(),\n            'model_config': {\n                'vocab_size': preprocessor.vocab_size,\n                'embedding_dim': 100,\n                'num_classes': num_classes,\n                'model_type': model_name\n            },\n            'test_metrics': results_dict[model_name]['test_metrics']\n        }, model_path)\n        print(f\"Saved {model_name} model to {model_path}\")\n    \n    # Save preprocessor\n    import pickle\n    with open('models/preprocessor.pkl', 'wb') as f:\n        pickle.dump(preprocessor, f)\n    print(\"Saved preprocessor\")\n    \n    # Save embedding matrix\n    np.save('embeddings/embedding_matrix.npy', embedding_matrix)\n    print(\"Saved embedding matrix\")\n    \n    # Save comprehensive results summary\n    summary = {\n        'models': list(models_dict.keys()),\n        'results': {\n            name: {\n                'test_accuracy': results['test_metrics']['accuracy'],\n                'test_f1': results['test_metrics']['f1'],\n                'test_loss': results['test_metrics']['loss'],\n                'training_epochs': len(results['history']['train_loss'])\n            }\n            for name, results in results_dict.items()\n        },\n        'dataset_info': {\n            'num_samples': len(df),\n            'num_classes': num_classes,\n            'vocab_size': preprocessor.vocab_size,\n            'max_seq_length': max_seq_length,\n            'embedding_dim': 100\n        },\n        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    }\n    \n    with open('results/all_dl_models_summary.json', 'w') as f:\n        json.dump(summary, f, indent=4)\n    print(\"\\nSaved results summary to results/all_dl_models_summary.json\")\n    \n    return summary\n\n# Save all models\nmodels_dict = {\n    'CNN': cnn_model,\n    'BiLSTM': bilstm_model,\n    'GRU': gru_model,\n    'CNN-LSTM': cnn_lstm_model\n}\n\nresults_dict = {\n    'CNN': cnn_results,\n    'BiLSTM': bilstm_results,\n    'GRU': gru_results,\n    'CNN-LSTM': cnn_lstm_results\n}\n\nsummary = save_all_dl_models(models_dict, results_dict, preprocessor)\n\n# Print final summary\nprint(\"\\n\" + \"=\"*80)\nprint(\"FINAL SUMMARY - ALL DEEP LEARNING MODELS\")\nprint(\"=\"*80)\nprint(f\"Dataset: {summary['dataset_info']['num_samples']} samples, {summary['dataset_info']['num_classes']} classes\")\nprint(f\"Vocabulary size: {summary['dataset_info']['vocab_size']}\")\nprint(f\"Max sequence length: {summary['dataset_info']['max_seq_length']}\")\nprint(f\"Embedding dimension: {summary['dataset_info']['embedding_dim']}\")\n\nprint(f\"\\nModel Performance Summary:\")\nprint(\"-\"*60)\nprint(f\"{'Model':<12} {'Accuracy':<12} {'F1 Score':<12} {'Loss':<12}\")\nprint(\"-\"*60)\n\n# Sort models by accuracy\nsorted_models = sorted(summary['results'].items(), \n                      key=lambda x: x[1]['test_accuracy'], \n                      reverse=True)\n\nfor model, metrics in sorted_models:\n    print(f\"{model:<12} {metrics['test_accuracy']:<12.4f} \"\n          f\"{metrics['test_f1']:<12.4f} {metrics['test_loss']:<12.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 16 - Inference Example\n","metadata":{}},{"cell_type":"code","source":"def predict_sentiment(text, model, preprocessor, model_name):\n    \"\"\"Predict sentiment for a given text\"\"\"\n    \n    # Preprocess text\n    sequence = preprocessor.text_to_indices(text, max_length=max_seq_length)\n    sequence_tensor = torch.tensor([sequence], dtype=torch.long).to(device)\n    \n    # Predict\n    model.eval()\n    with torch.no_grad():\n        outputs = model(sequence_tensor)\n        probs = F.softmax(outputs, dim=1)\n        _, predicted = torch.max(outputs, 1)\n    \n    # Get results\n    predicted_class = predicted.item()\n    confidence = probs[0][predicted_class].item()\n    \n    return {\n        'text': text,\n        'sentiment': class_names[predicted_class],\n        'confidence': confidence,\n        'probabilities': {class_names[i]: probs[0][i].item() for i in range(num_classes)}\n    }\n\n# Test examples\ntest_texts = [\n    \"এই পণ্যটি খুবই ভালো, আমি খুব সন্তুষ্ট।\",\n    \"সার্ভিস একদম বাজে, কখনো কিনবেন না।\",\n    \"মোটামুটি ঠিক আছে, দাম অনুযায়ী ভালো।\",\n    \"অসাধারণ! আমার খুব পছন্দ হয়েছে।\",\n    \"খুবই হতাশাজনক অভিজ্ঞতা।\"\n]\n\nprint(\"SENTIMENT PREDICTIONS\")\nprint(\"=\"*70)\n\nfor model_name, model in models_dict.items():\n    print(f\"\\n{model_name} Predictions:\")\n    print(\"-\"*50)\n    \n    for text in test_texts[:3]:  # Show first 3 examples\n        result = predict_sentiment(text, model, preprocessor, model_name)\n        print(f\"\\nText: {result['text']}\")\n        print(f\"Sentiment: {result['sentiment']} (Confidence: {result['confidence']:.4f})\")\n        print(f\"Probabilities: {', '.join([f'{k}: {v:.4f}' for k, v in result['probabilities'].items()])}\")\n\n# Performance comparison visualization\nprint(\"\\n\\nPERFORMANCE COMPARISON\")\nprint(\"=\"*70)\nprint(f\"{'Model':<10} {'Accuracy':<10} {'F1 Score':<10} {'Parameters':<15}\")\nprint(\"-\"*50)\nfor model_name, model in models_dict.items():\n    params = sum(p.numel() for p in model.parameters())\n    acc = results_dict[model_name]['test_metrics']['accuracy']\n    f1 = results_dict[model_name]['test_metrics']['f1']\n    print(f\"{model_name:<10} {acc:<10.4f} {f1:<10.4f} {params:<15,}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}