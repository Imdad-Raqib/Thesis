{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12250103,"sourceType":"datasetVersion","datasetId":7718676}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Aspect Based Sentiment Analysis Datasets for Bangla Text\n\n## Dataset Details\n\n### Dataset Characteristics\n- **Total Size**: 3,725 comments across 4 domains\n- **Source**: Manually created by undergraduate students\n- **Language**: Bangla text (complex/compound sentences)\n- **Unique Feature**: Each comment contains **2 aspects** with **2 sentiment polarities**\n- **Labeling**: Annotated by 3 annotators with majority voting\n\n### Domain-wise Distribution\n| Domain | Number of Comments | Average Length (words) | Max Length (words) |\n|--------|-------------------|------------------------|-------------------|\n| Car_ABSA | 1,149 | 10.26 | 19 |\n| Mobile_phone_ABSA | 975 | 10.14 | 20 |\n| Movie_ABSA | 800 | 11.78 | 35 |\n| Restaurant_ABSA | 801 | 11.22 | 23 |\n\n### Aspect Categories by Domain\n\n**Car_ABSA**: Performance, Exterior, Interior, Accessories, Comfort, Safety, Practicality, Others\n\n**Mobile_phone_ABSA**: Performance, Design, Display, Camera, Battery, Storage, Value_For_Money, Others\n\n**Movie_ABSA**: Story, Performance, Music, Miscellaneous\n\n**Restaurant_ABSA**: Food, Price, Service, Ambiance, Miscellaneous\n\n### Data Format\n- **Structure**: Id, Comment, {Aspect category, Sentiment Polarity}\n- **Sentiment Classes**: Positive, Negative\n\n## Best Performing Model\n\n### Model Architecture\n**Support Vector Machine (SVM)**\n\n### Performance Results\n\n#### Aspect Category Classification\n| Dataset | Precision (%) | Recall (%) | F1-Score (%) |\n|---------|---------------|------------|--------------|\n| Car_ABSA | 83.75 | 82.38 | **83.13** |\n| Mobile_phone_ABSA | 95.63 | 92.63 | **93.63** |\n| Movie_ABSA | 99.00 | 93.00 | **95.60** |\n| Restaurant_ABSA | 99.33 | 95.00 | **97.00** |\n\n#### Sentiment Polarity Classification\n| Dataset | Precision (%) | Recall (%) | F1-Score (%) |\n|---------|---------------|------------|--------------|\n| Car_ABSA | 84.50 | 84.00 | **84.00** |\n| Mobile_phone_ABSA | 82.50 | 82.00 | **82.50** |\n| Movie_ABSA | 94.33 | 78.33 | **83.67** |\n| Restaurant_ABSA | 99.33 | 92.33 | **95.67** |\n\n### Comparison with Existing Systems\n| Dataset | F1-Score |\n|---------|----------|\n| Cricket [1] | 0.34 |\n| Restaurant [1] | 0.38 |\n| BAN-ABSA [2] | 0.69 |\n| **Proposed Car_ABSA** | **0.83** |\n| **Proposed Mobile_Phone_ABSA** | **0.94** |\n| **Proposed Movie_ABSA** | **0.96** |\n| **Proposed Restaurant_ABSA** | **0.97** |\n\n","metadata":{}},{"cell_type":"markdown","source":"# Cell 1: Import necessary libraries\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_score, recall_score, f1_score, classification_report\nfrom sklearn.preprocessing import LabelEncoder\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 2: Load and explore the dataset\n","metadata":{}},{"cell_type":"code","source":"# Load your dataset\ndf = pd.read_csv('/kaggle/input/final-dataset/final-dataset.csv')\n\n# Display basic information\nprint(f\"Original dataset shape: {df.shape}\")\nprint(f\"\\nColumn names: {df.columns.tolist()}\")\nprint(f\"\\nSentiment distribution (before filtering):\")\nprint(df['Polarity'].value_counts())\n\n# Drop neutral comments to match the paper's setup\ndf = df[df['Polarity'] != 'neutral']\n\nprint(f\"\\nDataset shape after removing neutral: {df.shape}\")\nprint(f\"\\nSentiment distribution (after filtering):\")\nprint(df['Polarity'].value_counts())\nprint(f\"\\nSample data:\")\nprint(df.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 3: Define Bangla text preprocessing function\n","metadata":{}},{"cell_type":"code","source":"def preprocess_bangla_text(text):\n    \"\"\"Preprocess Bangla text according to the paper\"\"\"\n    if pd.isna(text):\n        return \"\"\n    \n    # Convert to string\n    text = str(text)\n    \n    # Remove specific punctuation marks mentioned in paper\n    text = re.sub(r'[:]', '', text)  # Remove colons\n    text = re.sub(r'[;]', '', text)  # Remove semicolons\n    \n    # Keep meaningful punctuation (,, !, ?, |)\n    # Remove multiple spaces\n    text = re.sub(r'\\s+', ' ', text)\n    \n    # Strip leading/trailing spaces\n    text = text.strip()\n    \n    return text\n\n# Bangla stop words from the paper\n# Adding more common Bangla stop words\nbangla_stop_words = [\n    'ęüđ', 'êąòČ', 'òì', 'öĐğČ', 'ąČě', 'ŁĕĘö',\n    'এই', 'সে', 'তা', 'এটা', 'সেটা', 'যে', 'তারা',\n    'আমি', 'তুমি', 'আমরা', 'তোমরা', 'এবং', 'ও',\n    'কিন্তু', 'যদি', 'তবে', 'নাহলে', 'অথবা'\n]\n\ndef remove_stop_words(text, stop_words):\n    \"\"\"Remove stop words from text\"\"\"\n    words = text.split()\n    filtered_words = [word for word in words if word not in stop_words]\n    return ' '.join(filtered_words)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 4: Apply preprocessing\n","metadata":{}},{"cell_type":"code","source":"# Apply preprocessing to Text column\ndf['processed_text'] = df['Text'].apply(preprocess_bangla_text)\ndf['processed_text'] = df['processed_text'].apply(lambda x: remove_stop_words(x, bangla_stop_words))\n\n# Remove empty comments after preprocessing\ndf = df[df['processed_text'].str.len() > 0]\n\nprint(f\"Dataset shape after preprocessing: {df.shape}\")\nprint(f\"\\nSample processed text:\")\nprint(df[['Text', 'processed_text']].head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 5: Prepare data for aspect category classification\n","metadata":{}},{"cell_type":"code","source":"# Create label encoder for sentiment\nsentiment_encoder = LabelEncoder()\n\n# Encode sentiment (positive -> 1, negative -> 0)\ndf['sentiment_encoded'] = sentiment_encoder.fit_transform(df['Polarity'])\n\n# Display encoding\nprint(\"Sentiment encoding:\")\nfor label, encoded in zip(sentiment_encoder.classes_, sentiment_encoder.transform(sentiment_encoder.classes_)):\n    print(f\"{label} -> {encoded}\")\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    df['processed_text'].values,\n    df['sentiment_encoded'].values,\n    test_size=0.2,\n    random_state=42,\n    stratify=df['sentiment_encoded']\n)\n\nprint(f\"\\nTraining set size: {len(X_train)}\")\nprint(f\"Test set size: {len(X_test)}\")\nprint(f\"Training set sentiment distribution:\")\nprint(pd.Series(y_train).value_counts())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 6: Create TF-IDF features\n","metadata":{}},{"cell_type":"code","source":"# Initialize TF-IDF Vectorizer\ntfidf_vectorizer = TfidfVectorizer(\n    max_features=5000,\n    ngram_range=(1, 2),  # Using unigrams and bigrams\n    min_df=2\n)\n\n# Fit and transform training data\nX_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\nX_test_tfidf = tfidf_vectorizer.transform(X_test)\n\nprint(f\"TF-IDF features shape: {X_train_tfidf.shape}\")\nprint(f\"Vocabulary size: {len(tfidf_vectorizer.vocabulary_)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 7: Train SVM model for aspect category classification\n","metadata":{}},{"cell_type":"code","source":"# Initialize SVM model with linear kernel (as per paper)\nsvm_model = SVC(kernel='linear', random_state=42, probability=True)\n\n# Train the model\nprint(\"Training SVM for sentiment classification...\")\nsvm_model.fit(X_train_tfidf, y_train)\n\n# Make predictions\ny_pred = svm_model.predict(X_test_tfidf)\n\n# Calculate metrics\nprecision = precision_score(y_test, y_pred, average='weighted')\nrecall = recall_score(y_test, y_pred, average='weighted')\nf1 = f1_score(y_test, y_pred, average='weighted')\n\nprint(f\"\\nSVM Sentiment Classification Results:\")\nprint(f\"Precision: {precision:.2%}\")\nprint(f\"Recall: {recall:.2%}\")\nprint(f\"F1-Score: {f1:.2%}\")\n\n# Detailed classification report\nprint(\"\\nDetailed Classification Report:\")\nprint(classification_report(y_test, y_pred, \n                          target_names=sentiment_encoder.classes_))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 8: Prepare data for sentiment polarity classification\n","metadata":{}},{"cell_type":"code","source":"# Naive Bayes\nnb_model = MultinomialNB()\nnb_model.fit(X_train_tfidf, y_train)\ny_pred_nb = nb_model.predict(X_test_tfidf)\n\nprecision_nb = precision_score(y_test, y_pred_nb, average='weighted')\nrecall_nb = recall_score(y_test, y_pred_nb, average='weighted')\nf1_nb = f1_score(y_test, y_pred_nb, average='weighted')\n\n# Logistic Regression\nlr_model = LogisticRegression(max_iter=1000, random_state=42)\nlr_model.fit(X_train_tfidf, y_train)\ny_pred_lr = lr_model.predict(X_test_tfidf)\n\nprecision_lr = precision_score(y_test, y_pred_lr, average='weighted')\nrecall_lr = recall_score(y_test, y_pred_lr, average='weighted')\nf1_lr = f1_score(y_test, y_pred_lr, average='weighted')\n\n# Display comparison\nprint(\"\\nModel Comparison:\")\nprint(f\"\\nNaive Bayes:\")\nprint(f\"  Precision: {precision_nb:.2%}\")\nprint(f\"  Recall: {recall_nb:.2%}\")\nprint(f\"  F1-Score: {f1_nb:.2%}\")\n\nprint(f\"\\nLogistic Regression:\")\nprint(f\"  Precision: {precision_lr:.2%}\")\nprint(f\"  Recall: {recall_lr:.2%}\")\nprint(f\"  F1-Score: {f1_lr:.2%}\")\n\nprint(f\"\\nSVM (Best Model):\")\nprint(f\"  Precision: {precision:.2%}\")\nprint(f\"  Recall: {recall:.2%}\")\nprint(f\"  F1-Score: {f1:.2%}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 9: Train SVM model for sentiment polarity classification\n","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Create confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Plot confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=sentiment_encoder.classes_,\n            yticklabels=sentiment_encoder.classes_)\nplt.title('Confusion Matrix - SVM Model')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.show()\n\n# Error analysis - look at misclassified examples\nmisclassified_idx = np.where(y_test != y_pred)[0]\nprint(f\"\\nNumber of misclassified samples: {len(misclassified_idx)}\")\nprint(f\"Misclassification rate: {len(misclassified_idx)/len(y_test):.2%}\")\n\n# Show some misclassified examples\nif len(misclassified_idx) > 0:\n    print(\"\\nSome misclassified examples:\")\n    for i in misclassified_idx[:5]:  # Show first 5\n        print(f\"\\nText: {X_test[i]}\")\n        print(f\"True label: {sentiment_encoder.inverse_transform([y_test[i]])[0]}\")\n        print(f\"Predicted: {sentiment_encoder.inverse_transform([y_pred[i]])[0]}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 10: Compare with other models (Naive Bayes and Logistic Regression)\n","metadata":{}},{"cell_type":"code","source":"import pickle\n\n# Save the best performing model (SVM)\nwith open('svm_sentiment_model.pkl', 'wb') as f:\n    pickle.dump(svm_model, f)\n\n# Save the TF-IDF vectorizer\nwith open('tfidf_vectorizer.pkl', 'wb') as f:\n    pickle.dump(tfidf_vectorizer, f)\n\n# Save the label encoder\nwith open('sentiment_encoder.pkl', 'wb') as f:\n    pickle.dump(sentiment_encoder, f)\n\n# Also save the preprocessing parameters\npreprocessing_params = {\n    'stop_words': bangla_stop_words,\n    'model_type': 'SVM',\n    'features': 'TF-IDF',\n    'ngram_range': (1, 2),\n    'max_features': 5000\n}\n\nwith open('preprocessing_params.pkl', 'wb') as f:\n    pickle.dump(preprocessing_params, f)\n\nprint(\"Models and preprocessing components saved successfully!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 11: Save the models and vectorizer\n","metadata":{}},{"cell_type":"code","source":"def predict_sentiment(text, tfidf_vectorizer, svm_model, sentiment_encoder):\n    \"\"\"Predict sentiment for new Bangla text\"\"\"\n    \n    # Preprocess the text\n    processed_text = preprocess_bangla_text(text)\n    processed_text = remove_stop_words(processed_text, bangla_stop_words)\n    \n    # Transform using TF-IDF\n    text_tfidf = tfidf_vectorizer.transform([processed_text])\n    \n    # Predict sentiment\n    sentiment_pred = svm_model.predict(text_tfidf)[0]\n    sentiment_proba = svm_model.predict_proba(text_tfidf)[0]\n    \n    # Get label\n    sentiment_label = sentiment_encoder.inverse_transform([sentiment_pred])[0]\n    \n    return {\n        'text': text,\n        'processed_text': processed_text,\n        'predicted_sentiment': sentiment_label,\n        'confidence': {\n            sentiment_encoder.inverse_transform([i])[0]: float(prob) \n            for i, prob in enumerate(sentiment_proba)\n        }\n    }\n\n# Test the function with some examples\ntest_texts = [\n    \"এই পণ্যটি খুবই ভালো\",\n    \"খুব খারাপ সার্ভিস, একদম পছন্দ হয়নি\",\n    \"দাম অনেক বেশি কিন্তু মান ভালো\"\n]\n\nprint(\"Testing sentiment prediction:\\n\")\nfor text in test_texts:\n    result = predict_sentiment(text, tfidf_vectorizer, svm_model, sentiment_encoder)\n    print(f\"Text: {result['text']}\")\n    print(f\"Predicted Sentiment: {result['predicted_sentiment']}\")\n    print(f\"Confidence: {result['confidence']}\")\n    print(\"-\" * 50)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 12: Function to predict both aspect and sentiment for new text\n","metadata":{}},{"cell_type":"code","source":"# Load saved components\nwith open('svm_sentiment_model.pkl', 'rb') as f:\n    loaded_svm = pickle.load(f)\n\nwith open('tfidf_vectorizer.pkl', 'rb') as f:\n    loaded_vectorizer = pickle.load(f)\n\nwith open('sentiment_encoder.pkl', 'rb') as f:\n    loaded_encoder = pickle.load(f)\n\n# Test with loaded model\nsample_text = \"অসাধারণ পণ্য, খুবই সন্তুষ্ট\"\nresult = predict_sentiment(sample_text, loaded_vectorizer, loaded_svm, loaded_encoder)\nprint(f\"Loaded model test:\")\nprint(f\"Text: {result['text']}\")\nprint(f\"Sentiment: {result['predicted_sentiment']}\")\nprint(f\"Confidence: {result['confidence']}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}