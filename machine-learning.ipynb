{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12250103,"sourceType":"datasetVersion","datasetId":7718676}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1 - Setup and Imports\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install required packages\n!pip install --upgrade pip setuptools wheel -q\n!pip install scikit-learn pandas numpy matplotlib seaborn tqdm -q\n!pip install nltk -q\n!pip install imbalanced-learn -q  # For SMOTE\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.auto import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ML libraries\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\nfrom sklearn.metrics import precision_recall_fscore_support, roc_auc_score\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# ML Models\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\n\n# Feature selection\nfrom sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n\n# Imbalanced data handling\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline as ImbPipeline\n\n# NLP Processing\nimport re\nimport string\nfrom collections import Counter, defaultdict\nimport nltk\n\n# Download NLTK data\nnltk.download('punkt', quiet=True)\nnltk.download('stopwords', quiet=True)\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\n# System utilities\nimport gc\nimport json\nimport random\nfrom datetime import datetime\nimport pickle\n\n# Set seeds for reproducibility\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed(42)\n\n# Create directories\nfor dir_name in ['plots', 'models', 'results']:\n    os.makedirs(dir_name, exist_ok=True)\n\nprint(\"Setup completed successfully!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 2 - Load and Analyze Dataset\n","metadata":{}},{"cell_type":"code","source":"# Load dataset\ndf = pd.read_csv('/kaggle/input/final-dataset/final-dataset.csv')\nprint(f\"Original dataset shape: {df.shape}\")\n\n# Check unique polarities to determine number of classes\nunique_polarities = df['Polarity'].unique()\nprint(f\"\\nUnique polarities found: {unique_polarities}\")\n\n# Map polarities based on what's in the dataset\nif 'neutral' in unique_polarities:\n    # 3-class case\n    num_classes = 3\n    df['Polarity'] = df['Polarity'].map({'positive': 1, 'negative': 0, 'neutral': 2})\n    class_names = {0: 'Negative', 1: 'Positive', 2: 'Neutral'}\n    print(\"\\nDetected 3-class sentiment analysis\")\nelse:\n    # 2-class case\n    num_classes = 2\n    df['Polarity'] = df['Polarity'].map({'positive': 1, 'negative': 0})\n    class_names = {0: 'Negative', 1: 'Positive'}\n    print(\"\\nDetected 2-class sentiment analysis\")\n\nprint(f\"Number of classes: {num_classes}\")\nprint(\"\\nClass distribution:\")\nclass_dist = df['Polarity'].value_counts().sort_index()\nprint(class_dist)\nprint(\"\\nClass percentages:\")\nprint(df['Polarity'].value_counts(normalize=True).sort_index() * 100)\n\n# Add text statistics\ndf['text_length'] = df['Text'].str.len()\ndf['word_count'] = df['Text'].str.split().str.len()\ndf['unique_words'] = df['Text'].apply(lambda x: len(set(str(x).split())))\n\nprint(f\"\\nText statistics:\")\nprint(f\"Average text length: {df['text_length'].mean():.1f} characters\")\nprint(f\"Average word count: {df['word_count'].mean():.1f} words\")\nprint(f\"Average unique words: {df['unique_words'].mean():.1f} words\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 3 - Advanced Text Preprocessing\n","metadata":{}},{"cell_type":"code","source":"class BanglaTextPreprocessor:\n    \"\"\"Advanced text preprocessing for Bangla text\"\"\"\n    \n    def __init__(self):\n        # Common Bangla punctuation\n        self.bangla_punct = '।॥,;:!?\\'\"-.…'\n        \n        # Common Bangla stopwords (expand this list based on your needs)\n        self.bangla_stopwords = {\n            'এবং', 'আর', 'কিন্তু', 'তবে', 'যদি', 'তাহলে', 'যে', 'সে', \n            'এই', 'সেই', 'ঐ', 'তার', 'তাদের', 'আমার', 'আমাদের',\n            'হয়', 'হবে', 'হল', 'করে', 'করা', 'করেন', 'করি', 'করেছে',\n            'ছিল', 'আছে', 'থাকে', 'যায়', 'যাবে', 'গেল', 'এর', 'তে',\n            'কে', 'না', 'নি', 'নেই', 'তো', 'ও', 'আর', 'কি', 'যা',\n            'জন্য', 'মত', 'সব', 'সে', 'এ', 'য়', 'র', 'ই'\n        }\n        \n    def clean_text(self, text):\n        \"\"\"Comprehensive cleaning for Bangla text\"\"\"\n        # Convert to string\n        text = str(text)\n        \n        # Convert to lowercase (if needed - be careful with Bangla)\n        # text = text.lower()\n        \n        # Remove URLs\n        text = re.sub(r'http\\S+|www.\\S+', '', text)\n        \n        # Remove email addresses\n        text = re.sub(r'\\S+@\\S+', '', text)\n        \n        # Remove English characters but keep Bangla\n        text = re.sub(r'[a-zA-Z]+', '', text)\n        \n        # Remove numbers (optional - comment out if numbers are important)\n        text = re.sub(r'\\d+', '', text)\n        \n        # Keep only Bangla characters and basic punctuation\n        text = re.sub(r'[^\\u0980-\\u09FF\\s।,!?.-]', ' ', text)\n        \n        # Remove extra whitespace\n        text = ' '.join(text.split())\n        \n        # Remove punctuation at the beginning and end\n        text = text.strip('।,!?.-')\n        \n        return text.strip()\n    \n    def tokenize(self, text):\n        \"\"\"Simple word tokenization for Bangla\"\"\"\n        # Clean text first\n        text = self.clean_text(text)\n        \n        # Simple split-based tokenization\n        tokens = text.split()\n        \n        # Remove very short tokens\n        tokens = [token for token in tokens if len(token) > 1]\n        \n        return tokens\n    \n    def remove_stopwords(self, tokens):\n        \"\"\"Remove stopwords from token list\"\"\"\n        return [token for token in tokens if token not in self.bangla_stopwords]\n    \n    def preprocess(self, text, remove_stop=True):\n        \"\"\"Complete preprocessing pipeline\"\"\"\n        # Clean text\n        text = self.clean_text(text)\n        \n        # Tokenize\n        tokens = self.tokenize(text)\n        \n        # Remove stopwords if requested\n        if remove_stop:\n            tokens = self.remove_stopwords(tokens)\n        \n        # Join tokens back\n        return ' '.join(tokens)\n    \n    def get_char_ngrams(self, text, n=3):\n        \"\"\"Extract character n-grams\"\"\"\n        text = self.clean_text(text)\n        ngrams = []\n        for i in range(len(text) - n + 1):\n            ngrams.append(text[i:i+n])\n        return ' '.join(ngrams)\n\n# Initialize preprocessor\npreprocessor = BanglaTextPreprocessor()\n\n# Test preprocessing\nprint(\"Preprocessing examples:\")\nsample_texts = df['Text'].head(3).values\nfor i, text in enumerate(sample_texts):\n    print(f\"\\nOriginal {i+1}: {text[:100]}...\")\n    processed = preprocessor.preprocess(text)\n    print(f\"Processed {i+1}: {processed[:100]}...\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 4 - Feature Engineering\n","metadata":{}},{"cell_type":"code","source":"def create_features(df, preprocessor):\n    \"\"\"Create various features for ML models\"\"\"\n    \n    print(\"Creating features...\")\n    \n    # Basic text features\n    df['processed_text'] = df['Text'].apply(lambda x: preprocessor.preprocess(x, remove_stop=True))\n    df['processed_text_with_stop'] = df['Text'].apply(lambda x: preprocessor.preprocess(x, remove_stop=False))\n    \n    # Character n-grams\n    df['char_trigrams'] = df['Text'].apply(lambda x: preprocessor.get_char_ngrams(x, n=3))\n    df['char_bigrams'] = df['Text'].apply(lambda x: preprocessor.get_char_ngrams(x, n=2))\n    \n    # Statistical features\n    df['char_count'] = df['Text'].str.len()\n    df['word_count'] = df['processed_text'].str.split().str.len()\n    df['unique_word_count'] = df['processed_text'].apply(lambda x: len(set(x.split())))\n    df['word_diversity'] = df['unique_word_count'] / (df['word_count'] + 1)  # +1 to avoid division by zero\n    \n    # Punctuation features\n    df['exclamation_count'] = df['Text'].str.count('!')\n    df['question_count'] = df['Text'].str.count('[?।]')\n    df['punctuation_count'] = df['Text'].apply(lambda x: sum(1 for c in x if c in '।!?,.-'))\n    \n    # Emoticon-like patterns (simplified)\n    df['happy_emoticon'] = df['Text'].str.count('[:;]-?[)D]')\n    df['sad_emoticon'] = df['Text'].str.count('[:;]-?[(]')\n    \n    # Average word length\n    df['avg_word_length'] = df['processed_text'].apply(\n        lambda x: np.mean([len(word) for word in x.split()]) if x else 0\n    )\n    \n    # Capitalization features (might not be as relevant for Bangla)\n    # df['capital_ratio'] = df['Text'].apply(lambda x: sum(1 for c in x if c.isupper()) / (len(x) + 1))\n    \n    print(f\"Created {len(df.columns)} features\")\n    return df\n\n# Create features\ndf = create_features(df, preprocessor)\n\n# Display feature statistics\nprint(\"\\nFeature statistics:\")\nnumerical_features = ['char_count', 'word_count', 'unique_word_count', 'word_diversity', \n                     'exclamation_count', 'question_count', 'punctuation_count', \n                     'avg_word_length']\nprint(df[numerical_features].describe())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 5 - Feature Extraction Methods\n","metadata":{}},{"cell_type":"code","source":"class FeatureExtractor:\n    \"\"\"Multiple feature extraction methods for text\"\"\"\n    \n    def __init__(self, max_features=5000):\n        self.max_features = max_features\n        self.vectorizers = {}\n        \n    def fit_transform_features(self, X_train, X_val, X_test, feature_type='tfidf'):\n        \"\"\"Extract features using specified method\"\"\"\n        \n        if feature_type == 'tfidf':\n            # TF-IDF with word-level features\n            vectorizer = TfidfVectorizer(\n                max_features=self.max_features,\n                ngram_range=(1, 3),  # Unigrams, bigrams, and trigrams\n                min_df=2,\n                max_df=0.95,\n                use_idf=True,\n                smooth_idf=True,\n                sublinear_tf=True  # Use log(TF)\n            )\n            \n        elif feature_type == 'tfidf_char':\n            # Character-level TF-IDF\n            vectorizer = TfidfVectorizer(\n                max_features=self.max_features,\n                analyzer='char',\n                ngram_range=(2, 5),  # Character n-grams\n                min_df=2,\n                max_df=0.95,\n                use_idf=True,\n                smooth_idf=True,\n                sublinear_tf=True\n            )\n            \n        elif feature_type == 'count':\n            # Count vectorizer\n            vectorizer = CountVectorizer(\n                max_features=self.max_features,\n                ngram_range=(1, 3),\n                min_df=2,\n                max_df=0.95,\n                binary=False\n            )\n            \n        elif feature_type == 'binary':\n            # Binary bag of words\n            vectorizer = CountVectorizer(\n                max_features=self.max_features,\n                ngram_range=(1, 2),\n                min_df=2,\n                max_df=0.95,\n                binary=True\n            )\n        \n        # Fit on training data and transform all sets\n        X_train_vec = vectorizer.fit_transform(X_train)\n        X_val_vec = vectorizer.transform(X_val)\n        X_test_vec = vectorizer.transform(X_test)\n        \n        self.vectorizers[feature_type] = vectorizer\n        \n        print(f\"{feature_type} features shape: {X_train_vec.shape}\")\n        \n        return X_train_vec, X_val_vec, X_test_vec\n    \n    def combine_features(self, text_features, numerical_features):\n        \"\"\"Combine text features with numerical features\"\"\"\n        from scipy.sparse import hstack\n        \n        # Convert numerical features to sparse matrix\n        from scipy.sparse import csr_matrix\n        numerical_sparse = csr_matrix(numerical_features)\n        \n        # Combine\n        combined = hstack([text_features, numerical_sparse])\n        \n        return combined\n\n# Initialize feature extractor\nfeature_extractor = FeatureExtractor(max_features=5000)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 6 - Train/Test Split and Feature Preparation\n","metadata":{}},{"cell_type":"code","source":"# Prepare data splits\ndef prepare_data_splits(df, test_size=0.2, val_size=0.1):\n    \"\"\"Prepare train, validation, and test splits\"\"\"\n    \n    # Features and labels\n    X_text = df['processed_text'].values\n    X_text_with_stop = df['processed_text_with_stop'].values\n    X_char = df['char_trigrams'].values\n    \n    # Numerical features\n    numerical_cols = ['char_count', 'word_count', 'unique_word_count', 'word_diversity',\n                      'exclamation_count', 'question_count', 'punctuation_count', 'avg_word_length']\n    X_numerical = df[numerical_cols].values\n    \n    y = df['Polarity'].values\n    \n    # First split: train+val and test\n    X_text_temp, X_text_test, X_numerical_temp, X_numerical_test, y_temp, y_test = train_test_split(\n        X_text, X_numerical, y, test_size=test_size, stratify=y, random_state=42\n    )\n    \n    # Also split other text representations\n    X_text_with_stop_temp, X_text_with_stop_test = train_test_split(\n        X_text_with_stop, test_size=test_size, stratify=y, random_state=42\n    )[0:2]\n    \n    X_char_temp, X_char_test = train_test_split(\n        X_char, test_size=test_size, stratify=y, random_state=42\n    )[0:2]\n    \n    # Second split: train and validation\n    val_size_adjusted = val_size / (1 - test_size)\n    \n    splits = train_test_split(\n        X_text_temp, X_text_with_stop_temp, X_char_temp, X_numerical_temp, y_temp,\n        test_size=val_size_adjusted, stratify=y_temp, random_state=42\n    )\n    \n    X_text_train = splits[0]\n    X_text_val = splits[1]\n    X_text_with_stop_train = splits[2]\n    X_text_with_stop_val = splits[3]\n    X_char_train = splits[4]\n    X_char_val = splits[5]\n    X_numerical_train = splits[6]\n    X_numerical_val = splits[7]\n    y_train = splits[8]\n    y_val = splits[9]\n    \n    print(\"Data split sizes:\")\n    print(f\"Train: {len(y_train)} samples\")\n    print(f\"Val: {len(y_val)} samples\")\n    print(f\"Test: {len(y_test)} samples\")\n    \n    print(\"\\nClass distribution:\")\n    print(f\"Train: {np.bincount(y_train)}\")\n    print(f\"Val: {np.bincount(y_val)}\")\n    print(f\"Test: {np.bincount(y_test)}\")\n    \n    return {\n        'text': (X_text_train, X_text_val, X_text_test),\n        'text_with_stop': (X_text_with_stop_train, X_text_with_stop_val, X_text_with_stop_test),\n        'char': (X_char_train, X_char_val, X_char_test),\n        'numerical': (X_numerical_train, X_numerical_val, X_numerical_test),\n        'labels': (y_train, y_val, y_test)\n    }\n\n# Prepare data\ndata_splits = prepare_data_splits(df)\ny_train, y_val, y_test = data_splits['labels']\n\n# Calculate class weights\nclass_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\nclass_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\nprint(f\"\\nClass weights: {class_weight_dict}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 7 - Extract Features\n","metadata":{}},{"cell_type":"code","source":"# Extract different types of features\nprint(\"Extracting features...\")\nprint(\"=\"*50)\n\n# 1. TF-IDF features\nprint(\"\\n1. TF-IDF Word Features:\")\nX_train_tfidf, X_val_tfidf, X_test_tfidf = feature_extractor.fit_transform_features(\n    *data_splits['text'], feature_type='tfidf'\n)\n\n# 2. TF-IDF character features\nprint(\"\\n2. TF-IDF Character Features:\")\nX_train_tfidf_char, X_val_tfidf_char, X_test_tfidf_char = feature_extractor.fit_transform_features(\n    *data_splits['char'], feature_type='tfidf_char'\n)\n\n# 3. Count features\nprint(\"\\n3. Count Features:\")\nX_train_count, X_val_count, X_test_count = feature_extractor.fit_transform_features(\n    *data_splits['text_with_stop'], feature_type='count'\n)\n\n# 4. Combine features\nprint(\"\\n4. Combining Features:\")\n\n# Scale numerical features\nscaler = StandardScaler()\nX_numerical_train_scaled = scaler.fit_transform(data_splits['numerical'][0])\nX_numerical_val_scaled = scaler.transform(data_splits['numerical'][1])\nX_numerical_test_scaled = scaler.transform(data_splits['numerical'][2])\n\n# Combine TF-IDF with numerical features\nX_train_combined = feature_extractor.combine_features(X_train_tfidf, X_numerical_train_scaled)\nX_val_combined = feature_extractor.combine_features(X_val_tfidf, X_numerical_val_scaled)\nX_test_combined = feature_extractor.combine_features(X_test_tfidf, X_numerical_test_scaled)\n\nprint(f\"Combined features shape: {X_train_combined.shape}\")\n\n# Store all feature sets\nfeature_sets = {\n    'tfidf': (X_train_tfidf, X_val_tfidf, X_test_tfidf),\n    'tfidf_char': (X_train_tfidf_char, X_val_tfidf_char, X_test_tfidf_char),\n    'count': (X_train_count, X_val_count, X_test_count),\n    'combined': (X_train_combined, X_val_combined, X_test_combined)\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 8 - SVM Implementation\n","metadata":{}},{"cell_type":"code","source":"def train_svm_models(feature_sets, y_train, y_val, y_test, class_weight_dict):\n    \"\"\"Train different SVM variants\"\"\"\n    \n    results = {}\n    \n    # SVM configurations\n    svm_configs = {\n        'LinearSVC': {\n            'model': LinearSVC(class_weight=class_weight_dict, random_state=42, max_iter=10000),\n            'param_grid': {\n                'C': [0.01, 0.1, 1.0, 10.0],\n                'penalty': ['l2'],\n                'loss': ['squared_hinge']\n            }\n        },\n        'SVC_RBF': {\n            'model': SVC(kernel='rbf', class_weight=class_weight_dict, random_state=42, \n                        probability=True, cache_size=2000),\n            'param_grid': {\n                'C': [0.1, 1.0, 10.0],\n                'gamma': ['scale', 'auto', 0.001, 0.01]\n            }\n        }\n    }\n    \n    for feature_name, (X_train, X_val, X_test) in feature_sets.items():\n        print(f\"\\n{'='*60}\")\n        print(f\"Training SVM models with {feature_name} features\")\n        print(f\"{'='*60}\")\n        \n        results[feature_name] = {}\n        \n        for svm_name, config in svm_configs.items():\n            print(f\"\\n{svm_name}:\")\n            \n            # Use smaller dataset for RBF kernel to speed up training\n            if svm_name == 'SVC_RBF' and X_train.shape[0] > 3000:\n                print(\"Using subset for RBF kernel due to computational constraints...\")\n                subset_idx = np.random.choice(X_train.shape[0], 3000, replace=False)\n                X_train_subset = X_train[subset_idx]\n                y_train_subset = y_train[subset_idx]\n            else:\n                X_train_subset = X_train\n                y_train_subset = y_train\n            \n            # Grid search with cross-validation\n            cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n            \n            grid_search = GridSearchCV(\n                config['model'],\n                config['param_grid'],\n                cv=cv,\n                scoring='f1_weighted',\n                n_jobs=-1,\n                verbose=1\n            )\n            \n            # Train\n            grid_search.fit(X_train_subset, y_train_subset)\n            \n            # Best model\n            best_model = grid_search.best_estimator_\n            \n            # Predictions\n            y_val_pred = best_model.predict(X_val)\n            y_test_pred = best_model.predict(X_test)\n            \n            # Metrics\n            val_acc = accuracy_score(y_val, y_val_pred)\n            val_f1 = f1_score(y_val, y_val_pred, average='weighted')\n            test_acc = accuracy_score(y_test, y_test_pred)\n            test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n            \n            print(f\"Best params: {grid_search.best_params_}\")\n            print(f\"Val - Accuracy: {val_acc:.4f}, F1: {val_f1:.4f}\")\n            print(f\"Test - Accuracy: {test_acc:.4f}, F1: {test_f1:.4f}\")\n            \n            # Store results\n            results[feature_name][svm_name] = {\n                'model': best_model,\n                'best_params': grid_search.best_params_,\n                'val_acc': val_acc,\n                'val_f1': val_f1,\n                'test_acc': test_acc,\n                'test_f1': test_f1,\n                'y_test_pred': y_test_pred\n            }\n    \n    return results\n\n# Train SVM models (using only best feature sets to save time)\nbest_feature_sets = {\n    'tfidf': feature_sets['tfidf'],\n    'combined': feature_sets['combined']\n}\n\nprint(\"Training SVM models...\")\nsvm_results = train_svm_models(best_feature_sets, y_train, y_val, y_test, class_weight_dict)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 9 - Random Forest Implementation\n","metadata":{}},{"cell_type":"code","source":"def train_random_forest_models(feature_sets, y_train, y_val, y_test, class_weight_dict):\n    \"\"\"Train Random Forest models with different configurations\"\"\"\n    \n    results = {}\n    \n    for feature_name, (X_train, X_val, X_test) in feature_sets.items():\n        print(f\"\\n{'='*60}\")\n        print(f\"Training Random Forest with {feature_name} features\")\n        print(f\"{'='*60}\")\n        \n        # Random Forest with grid search\n        rf = RandomForestClassifier(\n            class_weight=class_weight_dict,\n            random_state=42,\n            n_jobs=-1\n        )\n        \n        # Parameter grid\n        param_grid = {\n            'n_estimators': [100, 200, 300],\n            'max_depth': [10, 20, 30, None],\n            'min_samples_split': [2, 5, 10],\n            'min_samples_leaf': [1, 2, 4],\n            'max_features': ['sqrt', 'log2', None]\n        }\n        \n        # Simplified grid for faster training\n        param_grid_simple = {\n            'n_estimators': [100, 200],\n            'max_depth': [20, 30],\n            'min_samples_split': [5, 10],\n            'min_samples_leaf': [2, 4],\n            'max_features': ['sqrt', None]\n        }\n        \n        # Grid search\n        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n        \n        grid_search = GridSearchCV(\n            rf,\n            param_grid_simple,\n            cv=cv,\n            scoring='f1_weighted',\n            n_jobs=-1,\n            verbose=1\n        )\n        \n        # Train\n        grid_search.fit(X_train, y_train)\n        \n        # Best model\n        best_rf = grid_search.best_estimator_\n        \n        # Predictions\n        y_val_pred = best_rf.predict(X_val)\n        y_test_pred = best_rf.predict(X_test)\n        \n        # Get prediction probabilities\n        y_test_proba = best_rf.predict_proba(X_test)\n        \n        # Metrics\n        val_acc = accuracy_score(y_val, y_val_pred)\n        val_f1 = f1_score(y_val, y_val_pred, average='weighted')\n        test_acc = accuracy_score(y_test, y_test_pred)\n        test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n        \n        print(f\"Best params: {grid_search.best_params_}\")\n        print(f\"Val - Accuracy: {val_acc:.4f}, F1: {val_f1:.4f}\")\n        print(f\"Test - Accuracy: {test_acc:.4f}, F1: {test_f1:.4f}\")\n        \n        # Feature importance\n        feature_importance = best_rf.feature_importances_\n        \n        # Store results\n        results[feature_name] = {\n            'model': best_rf,\n            'best_params': grid_search.best_params_,\n            'val_acc': val_acc,\n            'val_f1': val_f1,\n            'test_acc': test_acc,\n            'test_f1': test_f1,\n            'y_test_pred': y_test_pred,\n            'y_test_proba': y_test_proba,\n            'feature_importance': feature_importance\n        }\n    \n    return results\n\n# Train Random Forest models\nprint(\"Training Random Forest models...\")\nrf_results = train_random_forest_models(best_feature_sets, y_train, y_val, y_test, class_weight_dict)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 10 - Additional ML Models (Quick Training)\n","metadata":{}},{"cell_type":"code","source":"def train_additional_models(X_train, X_val, X_test, y_train, y_val, y_test, class_weight_dict):\n    \"\"\"Train additional ML models for comparison\"\"\"\n    \n    models = {\n        'Logistic Regression': LogisticRegression(\n            class_weight=class_weight_dict,\n            max_iter=1000,\n            random_state=42,\n            solver='liblinear'\n        ),\n        'Multinomial NB': MultinomialNB(alpha=1.0),\n        'Extra Trees': ExtraTreesClassifier(\n            n_estimators=100,\n            class_weight=class_weight_dict,\n            random_state=42,\n            n_jobs=-1\n        )\n    }\n    \n    results = {}\n    \n    for name, model in models.items():\n        print(f\"\\nTraining {name}...\")\n        \n        # Train\n        model.fit(X_train, y_train)\n        \n        # Predictions\n        y_val_pred = model.predict(X_val)\n        y_test_pred = model.predict(X_test)\n        \n        # Metrics\n        val_acc = accuracy_score(y_val, y_val_pred)\n        val_f1 = f1_score(y_val, y_val_pred, average='weighted')\n        test_acc = accuracy_score(y_test, y_test_pred)\n        test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n        \n        print(f\"Val - Accuracy: {val_acc:.4f}, F1: {val_f1:.4f}\")\n        print(f\"Test - Accuracy: {test_acc:.4f}, F1: {test_f1:.4f}\")\n        \n        results[name] = {\n            'model': model,\n            'val_acc': val_acc,\n            'val_f1': val_f1,\n            'test_acc': test_acc,\n            'test_f1': test_f1,\n            'y_test_pred': y_test_pred\n        }\n    \n    return results\n\n# Train additional models with best features\nprint(\"\\nTraining additional ML models...\")\nprint(\"=\"*60)\nX_train_best, X_val_best, X_test_best = feature_sets['combined']\nadditional_results = train_additional_models(\n    X_train_best, X_val_best, X_test_best,\n    y_train, y_val, y_test, class_weight_dict\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 11 - Model Comparison and Visualization\n","metadata":{}},{"cell_type":"code","source":"def compare_all_models(svm_results, rf_results, additional_results, y_test):\n    \"\"\"Compare all trained models\"\"\"\n    \n    # Collect all results\n    all_results = []\n    \n    # SVM results\n    for feature_name, svm_dict in svm_results.items():\n        for svm_type, metrics in svm_dict.items():\n            all_results.append({\n                'Model': f'SVM {svm_type}',\n                'Features': feature_name,\n                'Test Accuracy': metrics['test_acc'],\n                'Test F1': metrics['test_f1'],\n                'Val Accuracy': metrics['val_acc'],\n                'Val F1': metrics['val_f1']\n            })\n    \n    # Random Forest results\n    for feature_name, metrics in rf_results.items():\n        all_results.append({\n            'Model': 'Random Forest',\n            'Features': feature_name,\n            'Test Accuracy': metrics['test_acc'],\n            'Test F1': metrics['test_f1'],\n            'Val Accuracy': metrics['val_acc'],\n            'Val F1': metrics['val_f1']\n        })\n    \n    # Additional models\n    for model_name, metrics in additional_results.items():\n        all_results.append({\n            'Model': model_name,\n            'Features': 'combined',\n            'Test Accuracy': metrics['test_acc'],\n            'Test F1': metrics['test_f1'],\n            'Val Accuracy': metrics['val_acc'],\n            'Val F1': metrics['val_f1']\n        })\n    \n    # Create DataFrame\n    results_df = pd.DataFrame(all_results)\n    results_df = results_df.sort_values('Test F1', ascending=False)\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"MODEL COMPARISON RESULTS\")\n    print(\"=\"*80)\n    print(results_df.to_string(index=False))\n    \n    # Visualization\n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n    \n    # 1. Model comparison bar plot\n    ax = axes[0, 0]\n    x = np.arange(len(results_df))\n    width = 0.35\n    \n    ax.bar(x - width/2, results_df['Test Accuracy'], width, label='Accuracy', color='skyblue')\n    ax.bar(x + width/2, results_df['Test F1'], width, label='F1 Score', color='lightcoral')\n    \n    ax.set_xlabel('Models')\n    ax.set_ylabel('Score')\n    ax.set_title('Model Performance Comparison')\n    ax.set_xticks(x)\n    ax.set_xticklabels([f\"{row['Model']}\\n({row['Features']})\" for _, row in results_df.iterrows()], \n                       rotation=45, ha='right')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    # 2. Best model confusion matrix\n    best_model_name = results_df.iloc[0]['Model']\n    best_feature = results_df.iloc[0]['Features']\n    \n    # Get predictions for best model\n    if 'SVM' in best_model_name:\n        svm_type = best_model_name.split()[-1]\n        y_pred = svm_results[best_feature][svm_type]['y_test_pred']\n    elif best_model_name == 'Random Forest':\n        y_pred = rf_results[best_feature]['y_test_pred']\n    else:\n        y_pred = additional_results[best_model_name]['y_test_pred']\n    \n    cm = confusion_matrix(y_test, y_pred)\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 1],\n                xticklabels=[class_names[i] for i in range(num_classes)],\n                yticklabels=[class_names[i] for i in range(num_classes)])\n    axes[0, 1].set_title(f'Best Model Confusion Matrix\\n({best_model_name} with {best_feature})')\n    axes[0, 1].set_ylabel('True Label')\n    axes[0, 1].set_xlabel('Predicted Label')\n    \n    # 3. Feature importance (if Random Forest)\n    if 'Random Forest' in [row['Model'] for _, row in results_df.iterrows()]:\n        ax = axes[1, 0]\n        \n        # Get feature importance\n        rf_feature = 'combined' if 'combined' in rf_results else 'tfidf'\n        feature_importance = rf_results[rf_feature]['feature_importance']\n        \n        # Get top 20 features\n        top_indices = np.argsort(feature_importance)[-20:]\n        top_importance = feature_importance[top_indices]\n        \n        ax.barh(np.arange(len(top_importance)), top_importance)\n        ax.set_xlabel('Feature Importance')\n        ax.set_title('Top 20 Feature Importances (Random Forest)')\n        ax.set_ylabel('Feature Index')\n    else:\n        axes[1, 0].text(0.5, 0.5, 'Feature importance not available', \n                        ha='center', va='center', transform=axes[1, 0].transAxes)\n        axes[1, 0].set_title('Feature Importance')\n    \n    # 4. Model comparison table\n    ax = axes[1, 1]\n    ax.axis('tight')\n    ax.axis('off')\n    \n    # Create summary table\n    summary_data = results_df[['Model', 'Features', 'Test Accuracy', 'Test F1']].head(5)\n    table = ax.table(cellText=summary_data.values,\n                     colLabels=summary_data.columns,\n                     cellLoc='center',\n                     loc='center')\n    table.auto_set_font_size(False)\n    table.set_fontsize(10)\n    table.scale(1.2, 1.5)\n    ax.set_title('Top 5 Models Summary', pad=20)\n    \n    plt.tight_layout()\n    plt.savefig('plots/ml_models_comparison.png', dpi=300, bbox_inches='tight')\n    plt.show()\n    \n    return results_df\n\n# Compare all models\nresults_summary = compare_all_models(svm_results, rf_results, additional_results, y_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 12 - Detailed Analysis of Best Models\n ","metadata":{}},{"cell_type":"code","source":"def detailed_model_analysis(model_results, model_name, X_test, y_test, feature_name='combined'):\n    \"\"\"Perform detailed analysis of a specific model\"\"\"\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"DETAILED ANALYSIS: {model_name}\")\n    print(f\"{'='*60}\")\n    \n    # Get model and predictions\n    if 'SVM' in model_name:\n        svm_type = model_name.split()[-1]\n        model_info = svm_results[feature_name][svm_type]\n    elif model_name == 'Random Forest':\n        model_info = rf_results[feature_name]\n    else:\n        model_info = additional_results[model_name]\n    \n    model = model_info['model']\n    y_pred = model_info['y_test_pred']\n    \n    # Classification report\n    print(\"\\nClassification Report:\")\n    print(classification_report(y_test, y_pred, \n                              target_names=[class_names[i] for i in range(num_classes)],\n                              digits=4))\n    \n    # Per-class performance\n    precision, recall, f1, support = precision_recall_fscore_support(\n        y_test, y_pred, average=None\n    )\n    \n    print(\"\\nPer-Class Performance:\")\n    print(f\"{'Class':<15} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Support':<10}\")\n    print(\"-\" * 55)\n    for i in range(num_classes):\n        print(f\"{class_names[i]:<15} {precision[i]:<10.4f} {recall[i]:<10.4f} \"\n              f\"{f1[i]:<10.4f} {support[i]:<10}\")\n    \n    # Error analysis\n    errors = y_pred != y_test\n    error_indices = np.where(errors)[0]\n    \n    print(f\"\\nError Analysis:\")\n    print(f\"Total errors: {len(error_indices)} ({len(error_indices)/len(y_test)*100:.2f}%)\")\n    \n    # Confusion patterns\n    print(\"\\nMost common confusion patterns:\")\n    confusion_patterns = {}\n    for idx in error_indices:\n        pattern = f\"{class_names[y_test[idx]]} → {class_names[y_pred[idx]]}\"\n        confusion_patterns[pattern] = confusion_patterns.get(pattern, 0) + 1\n    \n    for pattern, count in sorted(confusion_patterns.items(), key=lambda x: x[1], reverse=True)[:5]:\n        print(f\"{pattern}: {count} times\")\n    \n    # If model has predict_proba, analyze confidence\n    if hasattr(model, 'predict_proba'):\n        y_proba = model.predict_proba(X_test)\n        confidence = np.max(y_proba, axis=1)\n        \n        print(f\"\\nConfidence Analysis:\")\n        print(f\"Average confidence: {confidence.mean():.4f}\")\n        print(f\"Confidence on correct predictions: {confidence[~errors].mean():.4f}\")\n        print(f\"Confidence on errors: {confidence[errors].mean():.4f}\")\n        \n        # Low confidence predictions\n        low_conf_threshold = 0.5\n        low_conf = confidence < low_conf_threshold\n        print(f\"\\nPredictions with confidence < {low_conf_threshold}: \"\n              f\"{low_conf.sum()} ({low_conf.sum()/len(y_test)*100:.2f}%)\")\n\n# Analyze top 2 models\ntop_models = results_summary.head(2)\nfor _, row in top_models.iterrows():\n    model_name = row['Model']\n    feature_name = row['Features']\n    \n    if 'SVM' in model_name and feature_name in feature_sets:\n        X_test_model = feature_sets[feature_name][2]\n    elif feature_name in feature_sets:\n        X_test_model = feature_sets[feature_name][2]\n    else:\n        X_test_model = X_test_best\n    \n    detailed_model_analysis(None, model_name, X_test_model, y_test, feature_name)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 13 - Feature Analysis\n","metadata":{}},{"cell_type":"code","source":"def analyze_important_features(vectorizer, feature_importance=None, top_n=30):\n    \"\"\"Analyze most important features/words\"\"\"\n    \n    print(f\"\\n{'='*60}\")\n    print(\"FEATURE ANALYSIS\")\n    print(f\"{'='*60}\")\n    \n    # Get feature names\n    feature_names = vectorizer.get_feature_names_out()\n    \n    if feature_importance is not None:\n        # For tree-based models with feature importance\n        indices = np.argsort(feature_importance)[-top_n:][::-1]\n        \n        print(f\"\\nTop {top_n} important features:\")\n        print(f\"{'Rank':<6} {'Feature':<30} {'Importance':<10}\")\n        print(\"-\" * 46)\n        \n        for i, idx in enumerate(indices):\n            if idx < len(feature_names):\n                print(f\"{i+1:<6} {feature_names[idx]:<30} {feature_importance[idx]:<10.6f}\")\n    \n    # Analyze features by class (for TF-IDF)\n    if hasattr(vectorizer, 'idf_'):\n        print(f\"\\n\\nTop words by TF-IDF score:\")\n        \n        # Get TF-IDF scores\n        tfidf_scores = dict(zip(feature_names, vectorizer.idf_))\n        sorted_words = sorted(tfidf_scores.items(), key=lambda x: x[1], reverse=True)\n        \n        print(f\"{'Word':<30} {'TF-IDF Score':<10}\")\n        print(\"-\" * 40)\n        for word, score in sorted_words[:20]:\n            print(f\"{word:<30} {score:<10.4f}\")\n\n# Analyze features for best model\nif 'Random Forest' in results_summary.iloc[0]['Model']:\n    best_feature = results_summary.iloc[0]['Features']\n    if best_feature in feature_extractor.vectorizers:\n        analyze_important_features(\n            feature_extractor.vectorizers['tfidf'],\n            rf_results[best_feature]['feature_importance']\n        )\nelse:\n    # Analyze TF-IDF features\n    if 'tfidf' in feature_extractor.vectorizers:\n        analyze_important_features(feature_extractor.vectorizers['tfidf'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 14 - Save Models and Results\n","metadata":{}},{"cell_type":"code","source":"def save_ml_models(best_models, vectorizers, preprocessor, scaler):\n    \"\"\"Save best ML models and preprocessing components\"\"\"\n    \n    # Create a directory for ML models\n    ml_model_dir = 'models/ml_models'\n    os.makedirs(ml_model_dir, exist_ok=True)\n    \n    # Save best model\n    best_model_info = best_models.iloc[0]\n    model_name = best_model_info['Model']\n    feature_name = best_model_info['Features']\n    \n    # Get the actual model object\n    if 'SVM' in model_name:\n        svm_type = model_name.split()[-1]\n        model = svm_results[feature_name][svm_type]['model']\n    elif model_name == 'Random Forest':\n        model = rf_results[feature_name]['model']\n    else:\n        model = additional_results[model_name]['model']\n    \n    # Save model\n    model_filename = f\"{ml_model_dir}/best_model_{model_name.lower().replace(' ', '_')}.pkl\"\n    with open(model_filename, 'wb') as f:\n        pickle.dump(model, f)\n    print(f\"Saved model: {model_filename}\")\n    \n    # Save vectorizer\n    vectorizer = feature_extractor.vectorizers.get('tfidf')\n    if vectorizer:\n        with open(f\"{ml_model_dir}/tfidf_vectorizer.pkl\", 'wb') as f:\n            pickle.dump(vectorizer, f)\n        print(f\"Saved vectorizer: {ml_model_dir}/tfidf_vectorizer.pkl\")\n    \n    # Save preprocessor\n    with open(f\"{ml_model_dir}/preprocessor.pkl\", 'wb') as f:\n        pickle.dump(preprocessor, f)\n    print(f\"Saved preprocessor: {ml_model_dir}/preprocessor.pkl\")\n    \n    # Save scaler\n    with open(f\"{ml_model_dir}/scaler.pkl\", 'wb') as f:\n        pickle.dump(scaler, f)\n    print(f\"Saved scaler: {ml_model_dir}/scaler.pkl\")\n    \n    # Save results summary\n    results_summary = {\n        'best_model': {\n            'name': model_name,\n            'features': feature_name,\n            'test_accuracy': float(best_model_info['Test Accuracy']),\n            'test_f1': float(best_model_info['Test F1'])\n        },\n        'all_results': best_models.to_dict('records'),\n        'dataset_info': {\n            'num_samples': len(df),\n            'num_classes': num_classes,\n            'class_names': class_names\n        },\n        'feature_info': {\n            'max_features': feature_extractor.max_features,\n            'numerical_features': numerical_cols\n        },\n        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    }\n    \n    with open('results/ml_models_summary.json', 'w') as f:\n        json.dump(results_summary, f, indent=4)\n    print(f\"\\nSaved results summary: results/ml_models_summary.json\")\n    \n    return results_summary\n\n# Save models\nprint(\"\\nSaving models and results...\")\nnumerical_cols = ['char_count', 'word_count', 'unique_word_count', 'word_diversity',\n                  'exclamation_count', 'question_count', 'punctuation_count', 'avg_word_length']\nsaved_summary = save_ml_models(results_summary, feature_extractor.vectorizers, preprocessor, scaler)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 15 - Inference Pipeline\n","metadata":{}},{"cell_type":"code","source":"class MLSentimentPredictor:\n    \"\"\"Inference pipeline for ML models\"\"\"\n    \n    def __init__(self, model_path, vectorizer_path, preprocessor_path, scaler_path):\n        # Load components\n        with open(model_path, 'rb') as f:\n            self.model = pickle.load(f)\n        \n        with open(vectorizer_path, 'rb') as f:\n            self.vectorizer = pickle.load(f)\n        \n        with open(preprocessor_path, 'rb') as f:\n            self.preprocessor = pickle.load(f)\n        \n        with open(scaler_path, 'rb') as f:\n            self.scaler = pickle.load(f)\n        \n        self.class_names = {0: 'Negative', 1: 'Positive', 2: 'Neutral'}\n        self.num_classes = len(self.class_names)\n    \n    def extract_numerical_features(self, text):\n        \"\"\"Extract numerical features from text\"\"\"\n        processed = self.preprocessor.preprocess(text)\n        \n        features = {\n            'char_count': len(text),\n            'word_count': len(processed.split()),\n            'unique_word_count': len(set(processed.split())),\n            'word_diversity': len(set(processed.split())) / (len(processed.split()) + 1),\n            'exclamation_count': text.count('!'),\n            'question_count': len(re.findall('[?।]', text)),\n            'punctuation_count': sum(1 for c in text if c in '।!?,.-'),\n            'avg_word_length': np.mean([len(word) for word in processed.split()]) if processed else 0\n        }\n        \n        return np.array([features[col] for col in [\n            'char_count', 'word_count', 'unique_word_count', 'word_diversity',\n            'exclamation_count', 'question_count', 'punctuation_count', 'avg_word_length'\n        ]])\n    \n    def predict(self, text):\n        \"\"\"Predict sentiment for a single text\"\"\"\n        # Preprocess\n        processed_text = self.preprocessor.preprocess(text)\n        \n        # Extract text features\n        text_features = self.vectorizer.transform([processed_text])\n        \n        # Extract numerical features\n        num_features = self.extract_numerical_features(text).reshape(1, -1)\n        num_features_scaled = self.scaler.transform(num_features)\n        \n        # Combine features\n        from scipy.sparse import hstack, csr_matrix\n        combined_features = hstack([text_features, csr_matrix(num_features_scaled)])\n        \n        # Predict\n        prediction = self.model.predict(combined_features)[0]\n        \n        # Get probabilities if available\n        if hasattr(self.model, 'predict_proba'):\n            probabilities = self.model.predict_proba(combined_features)[0]\n            confidence = probabilities.max()\n        else:\n            probabilities = None\n            confidence = None\n        \n        result = {\n            'text': text,\n            'sentiment': self.class_names[prediction],\n            'label': prediction,\n            'confidence': confidence\n        }\n        \n        if probabilities is not None:\n            result['probabilities'] = {\n                self.class_names[i]: float(probabilities[i]) \n                for i in range(len(probabilities))\n            }\n        \n        return result\n    \n    def predict_batch(self, texts):\n        \"\"\"Predict sentiments for multiple texts\"\"\"\n        results = []\n        for text in texts:\n            results.append(self.predict(text))\n        return results\n\n# Initialize predictor\nml_predictor = MLSentimentPredictor(\n    model_path='models/ml_models/best_model_svm_linearsvc.pkl',  # Update based on best model\n    vectorizer_path='models/ml_models/tfidf_vectorizer.pkl',\n    preprocessor_path='models/ml_models/preprocessor.pkl',\n    scaler_path='models/ml_models/scaler.pkl'\n)\n\n# Test predictions\ntest_texts = [\n    \"এই পণ্যটি খুবই ভালো, আমি খুব সন্তুষ্ট।\",\n    \"সার্ভিস একদম বাজে, কখনো কিনবেন না।\",\n    \"মোটামুটি ঠিক আছে, দাম অনুযায়ী ভালো।\",\n    \"অসাধারণ! আমার খুব পছন্দ হয়েছে।\",\n    \"খুবই হতাশাজনক অভিজ্ঞতা।\"\n]\n\nprint(\"\\nSENTIMENT PREDICTIONS (ML Model)\")\nprint(\"=\"*70)\n\nfor text in test_texts:\n    result = ml_predictor.predict(text)\n    print(f\"\\nText: {result['text']}\")\n    print(f\"Sentiment: {result['sentiment']}\")\n    if result['confidence']:\n        print(f\"Confidence: {result['confidence']:.4f}\")\n    if 'probabilities' in result:\n        prob_str = \", \".join([f\"{k}: {v:.3f}\" for k, v in result['probabilities'].items()])\n        print(f\"Probabilities: {prob_str}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 16 - Final Summary\n","metadata":{}},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*80)\nprint(\"MACHINE LEARNING MODELS - FINAL SUMMARY\")\nprint(\"=\"*80)\n\n# Display results\nprint(f\"\\nDataset: {len(df)} samples, {num_classes} classes\")\nprint(f\"Train/Val/Test split: {len(y_train)}/{len(y_val)}/{len(y_test)}\")\nprint(f\"\\nBest performing models:\")\nprint(results_summary.head(5).to_string(index=False))\n\n# Compare with deep learning results (if available)\nprint(\"\\n\\nMODEL TYPE COMPARISON:\")\nprint(\"-\"*50)\nprint(f\"Best ML Model: {results_summary.iloc[0]['Model']}\")\nprint(f\"  - Accuracy: {results_summary.iloc[0]['Test Accuracy']:.4f}\")\nprint(f\"  - F1 Score: {results_summary.iloc[0]['Test F1']:.4f}\")\n\nprint(\"\\n\\nKEY INSIGHTS:\")\nprint(\"-\"*50)\nprint(\"1. Feature engineering is crucial for ML models\")\nprint(\"2. TF-IDF with n-grams performs well for Bangla text\")\nprint(\"3. Combining text and numerical features improves performance\")\nprint(\"4. Class balancing helps with imbalanced datasets\")\nprint(\"5. Simple models like LinearSVC can be very effective\")\n\nprint(\"\\n\\nRECOMMENDATIONS:\")\nprint(\"-\"*50)\nprint(\"1. For production: Use LinearSVC or Logistic Regression (fast inference)\")\nprint(\"2. For best accuracy: Use Random Forest or SVM with RBF kernel\")\nprint(\"3. Consider ensemble methods combining multiple models\")\nprint(\"4. Regular retraining with new data is important\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}