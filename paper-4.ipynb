{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12250103,"sourceType":"datasetVersion","datasetId":7718676}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Comparative Study of Transformer-Based Models and Bi-LSTM for Bangla Sentiment Analysis Using Hybrid Optimizers\n\n## Dataset Details\n\n### Dataset Characteristics\n- **Total Size**: 35,000 labeled text samples\n- **Source**: Social media platforms, news portals, and product reviews\n- **Language**: Bangla (Bengali)\n- **Distribution**: Balanced across three sentiment categories\n\n### Class Distribution\n| Sentiment Class | Number of Samples |\n|-----------------|-------------------|\n| Positive | 11,700 |\n| Negative | 11,600 |\n| Neutral | 11,700 |\n| **Total** | **35,000** |\n\n### Data Split\n- **Training**: 80% (28,000 samples)\n- **Validation**: 10% (3,500 samples)\n- **Testing**: 10% (3,500 samples)\n\n### Preprocessing Steps\n- Tokenization (Bangla-specific)\n- Lowercasing\n- Stopword removal\n- Punctuation and special character removal\n- Text normalization\n\n## Best Performing Model\n\n### Model Architecture\n**Bangla-RoBERTa with Hybrid Optimizer (AdamW + Lookahead)**\n\n### Model Configuration\n| Parameter | Value |\n|-----------|-------|\n| Model | Bangla-RoBERTa |\n| Max Sequence Length | 128 tokens |\n| Optimizer | Hybrid (AdamW + Lookahead) |\n| Learning Rate | 2e-5 |\n| Batch Size | 32 |\n| Epochs | 4 |\n| Dropout Rate | 0.5 |\n\n### Performance Results\n\n#### Model Comparison\n| Model | Optimizer | Accuracy (%) | F1-Score | Convergence Speed (Epochs) |\n|-------|-----------|--------------|----------|---------------------------|\n| Bi-LSTM | Adam | 84.7 | 0.825 | 12 |\n| Bi-LSTM | Hybrid | 86.3 | 0.841 | 9 |\n| BERT | Adam | 90.5 | 0.894 | 8 |\n| BERT | Hybrid | 91.8 | 0.909 | 6 |\n| RoBERTa | Adam | 92.3 | 0.917 | 7 |\n| **RoBERTa** | **Hybrid** | **93.6** | **0.931** | **5** |\n\n### Key Findings\n- Transformer models (BERT/RoBERTa) significantly outperform Bi-LSTM\n- Hybrid optimizers improve both accuracy and convergence speed\n- RoBERTa with hybrid optimizer achieves the best performance: **93.6% accuracy**\n- Fastest convergence: Only 5 epochs needed with hybrid optimizer\n\n### Confusion Matrix (Best Model)\n| Actual/Predicted | Positive | Negative |\n|------------------|----------|----------|\n| Positive | 890 (TP) | 110 (FN) |\n| Negative | 90 (FP) | 910 (TN) |\n\n### Resource Efficiency\n- **Bi-LSTM**: Lower memory usage, faster training time\n- **RoBERTa**: Higher memory requirements but superior performance\n- Trade-off between performance and computational resources","metadata":{}},{"cell_type":"markdown","source":"# Cell 1: Import necessary libraries\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForSequenceClassification,\n    AdamW,\n    get_linear_schedule_with_warmup\n)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Check if GPU is available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 2: Implement Lookahead Optimizer\n","metadata":{}},{"cell_type":"code","source":"class Lookahead(object):\n    \"\"\"Lookahead optimizer implementation\"\"\"\n    def __init__(self, base_optimizer, alpha=0.5, k=5):\n        self.optimizer = base_optimizer\n        self.alpha = alpha\n        self.k = k\n        self.step_count = 0\n        self.slow_weights = [[p.clone().detach() for p in group['params']] \n                            for group in self.optimizer.param_groups]\n        \n    def step(self, closure=None):\n        loss = self.optimizer.step(closure)\n        self.step_count += 1\n        \n        if self.step_count % self.k == 0:\n            for group, slow_weights in zip(self.optimizer.param_groups, self.slow_weights):\n                for p, q in zip(group['params'], slow_weights):\n                    q.data.add_(self.alpha, p.data - q.data)\n                    p.data.copy_(q.data)\n        return loss\n    \n    def state_dict(self):\n        return self.optimizer.state_dict()\n    \n    def load_state_dict(self, state_dict):\n        self.optimizer.load_state_dict(state_dict)\n        \n    def zero_grad(self):\n        self.optimizer.zero_grad()\n        \n    @property\n    def param_groups(self):\n        return self.optimizer.param_groups","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 3: Load and preprocess dataset\n","metadata":{}},{"cell_type":"code","source":"# Load your dataset\ndf = pd.read_csv('/kaggle/input/final-dataset/final-dataset.csv')\n\n# Display dataset info\nprint(f\"Dataset shape: {df.shape}\")\nprint(f\"\\nSentiment distribution:\")\nprint(df['Polarity'].value_counts())\n\n# Preprocessing function\ndef preprocess_bangla_text(text):\n    \"\"\"Preprocess Bangla text according to the paper\"\"\"\n    if pd.isna(text):\n        return \"\"\n    \n    text = str(text)\n    # Convert to lowercase\n    text = text.lower()\n    \n    # Remove extra spaces\n    text = ' '.join(text.split())\n    \n    return text\n\n# Apply preprocessing\ndf['processed_text'] = df['Text'].apply(preprocess_bangla_text)\n\n# Remove empty texts\ndf = df[df['processed_text'].str.len() > 0]\n\n# Encode labels\nlabel_mapping = {'positive': 0, 'negative': 1, 'neutral': 2}\ndf['label'] = df['Polarity'].map(label_mapping)\n\nprint(f\"\\nProcessed dataset shape: {df.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 4: Create PyTorch Dataset\n","metadata":{}},{"cell_type":"code","source":"class BanglaSentimentDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        label = self.labels[idx]\n        \n        encoding = self.tokenizer(\n            text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        \n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'label': torch.tensor(label, dtype=torch.long)\n        }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 5: Split data and create data loaders\n","metadata":{}},{"cell_type":"code","source":"# Split data\nX_train, X_temp, y_train, y_temp = train_test_split(\n    df['processed_text'].values, \n    df['label'].values, \n    test_size=0.2, \n    random_state=42,\n    stratify=df['label']\n)\n\nX_val, X_test, y_val, y_test = train_test_split(\n    X_temp, \n    y_temp, \n    test_size=0.5, \n    random_state=42,\n    stratify=y_temp\n)\n\nprint(f\"Training set: {len(X_train)}\")\nprint(f\"Validation set: {len(X_val)}\")\nprint(f\"Test set: {len(X_test)}\")\n\n# Load tokenizer\n# Note: Replace with actual Bangla-RoBERTa model name\nmodel_name = \"csebuetnlp/banglishbert\"  # Using BanglishBERT as proxy\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Create datasets\ntrain_dataset = BanglaSentimentDataset(X_train, y_train, tokenizer)\nval_dataset = BanglaSentimentDataset(X_val, y_val, tokenizer)\ntest_dataset = BanglaSentimentDataset(X_test, y_test, tokenizer)\n\n# Create data loaders\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 6: Load and configure the model\n","metadata":{}},{"cell_type":"code","source":"# Load pre-trained model\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name,\n    num_labels=3,  # 3 sentiment classes\n    output_attentions=False,\n    output_hidden_states=False\n)\n\nmodel.to(device)\n\n# Configure optimizer parameters\nlearning_rate = 2e-5\nnum_epochs = 4\nwarmup_steps = len(train_loader) * 2  # Warmup for 2 epochs\n\n# Create AdamW optimizer\noptimizer_base = AdamW(\n    model.parameters(),\n    lr=learning_rate,\n    eps=1e-8,\n    weight_decay=0.01  # AdamW includes weight decay\n)\n\n# Wrap with Lookahead\noptimizer = Lookahead(optimizer_base, alpha=0.5, k=5)\n\n# Create learning rate scheduler\ntotal_steps = len(train_loader) * num_epochs\nscheduler = get_linear_schedule_with_warmup(\n    optimizer_base,  # Use base optimizer for scheduler\n    num_warmup_steps=warmup_steps,\n    num_training_steps=total_steps\n)\n\nprint(f\"Model loaded: {model_name}\")\nprint(f\"Total training steps: {total_steps}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 7: Define training and evaluation functions\n","metadata":{}},{"cell_type":"code","source":"def train_epoch(model, data_loader, optimizer, scheduler, device):\n    model.train()\n    total_loss = 0\n    predictions = []\n    true_labels = []\n    \n    for batch in tqdm(data_loader, desc=\"Training\"):\n        # Move batch to device\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label'].to(device)\n        \n        # Zero gradients\n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n        \n        loss = outputs.loss\n        total_loss += loss.item()\n        \n        # Backward pass\n        loss.backward()\n        \n        # Clip gradients\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        \n        # Update weights\n        optimizer.step()\n        scheduler.step()\n        \n        # Get predictions\n        _, preds = torch.max(outputs.logits, dim=1)\n        predictions.extend(preds.cpu().numpy())\n        true_labels.extend(labels.cpu().numpy())\n    \n    avg_loss = total_loss / len(data_loader)\n    accuracy = accuracy_score(true_labels, predictions)\n    f1 = f1_score(true_labels, predictions, average='weighted')\n    \n    return avg_loss, accuracy, f1\n\ndef evaluate(model, data_loader, device):\n    model.eval()\n    total_loss = 0\n    predictions = []\n    true_labels = []\n    \n    with torch.no_grad():\n        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n            \n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                labels=labels\n            )\n            \n            loss = outputs.loss\n            total_loss += loss.item()\n            \n            _, preds = torch.max(outputs.logits, dim=1)\n            predictions.extend(preds.cpu().numpy())\n            true_labels.extend(labels.cpu().numpy())\n    \n    avg_loss = total_loss / len(data_loader)\n    accuracy = accuracy_score(true_labels, predictions)\n    f1 = f1_score(true_labels, predictions, average='weighted')\n    \n    return avg_loss, accuracy, f1, predictions, true_labels","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 8: Train the model\n","metadata":{}},{"cell_type":"code","source":"# Training loop\ntrain_losses = []\nval_losses = []\ntrain_accuracies = []\nval_accuracies = []\ntrain_f1_scores = []\nval_f1_scores = []\n\nbest_val_f1 = 0\nbest_model_state = None\n\nfor epoch in range(num_epochs):\n    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n    print(\"-\" * 30)\n    \n    # Train\n    train_loss, train_acc, train_f1 = train_epoch(\n        model, train_loader, optimizer, scheduler, device\n    )\n    \n    # Validate\n    val_loss, val_acc, val_f1, _, _ = evaluate(model, val_loader, device)\n    \n    # Store metrics\n    train_losses.append(train_loss)\n    val_losses.append(val_loss)\n    train_accuracies.append(train_acc)\n    val_accuracies.append(val_acc)\n    train_f1_scores.append(train_f1)\n    val_f1_scores.append(val_f1)\n    \n    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Train F1: {train_f1:.4f}\")\n    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f}\")\n    \n    # Save best model\n    if val_f1 > best_val_f1:\n        best_val_f1 = val_f1\n        best_model_state = model.state_dict().copy()\n        print(\"New best model saved!\")\n\n# Load best model\nmodel.load_state_dict(best_model_state)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 9: Plot training history\n","metadata":{}},{"cell_type":"code","source":"# Plot training history\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Loss plot\naxes[0].plot(train_losses, label='Train Loss')\naxes[0].plot(val_losses, label='Val Loss')\naxes[0].set_title('Model Loss')\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Loss')\naxes[0].legend()\n\n# Accuracy plot\naxes[1].plot(train_accuracies, label='Train Accuracy')\naxes[1].plot(val_accuracies, label='Val Accuracy')\naxes[1].set_title('Model Accuracy')\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Accuracy')\naxes[1].legend()\n\n# F1-Score plot\naxes[2].plot(train_f1_scores, label='Train F1')\naxes[2].plot(val_f1_scores, label='Val F1')\naxes[2].set_title('Model F1-Score')\naxes[2].set_xlabel('Epoch')\naxes[2].set_ylabel('F1-Score')\naxes[2].legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 10: Evaluate on test set\n","metadata":{}},{"cell_type":"code","source":"# Evaluate on test set\ntest_loss, test_acc, test_f1, test_preds, test_labels = evaluate(\n    model, test_loader, device\n)\n\nprint(f\"\\nTest Results:\")\nprint(f\"Test Loss: {test_loss:.4f}\")\nprint(f\"Test Accuracy: {test_acc:.4f}\")\nprint(f\"Test F1-Score: {test_f1:.4f}\")\n\n# Classification report\nlabel_names = ['Positive', 'Negative', 'Neutral']\nprint(\"\\nClassification Report:\")\nprint(classification_report(test_labels, test_preds, target_names=label_names))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 11: Confusion Matrix\n","metadata":{}},{"cell_type":"code","source":"# Create confusion matrix\ncm = confusion_matrix(test_labels, test_preds)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=label_names, yticklabels=label_names)\nplt.title('Confusion Matrix - RoBERTa with Hybrid Optimizer')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.show()\n\n# Calculate per-class metrics\nfor i, label in enumerate(label_names):\n    precision = cm[i, i] / cm[:, i].sum() if cm[:, i].sum() > 0 else 0\n    recall = cm[i, i] / cm[i, :].sum() if cm[i, :].sum() > 0 else 0\n    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n    print(f\"{label}: Precision={precision:.3f}, Recall={recall:.3f}, F1={f1:.3f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 12: Save the model\n","metadata":{}},{"cell_type":"code","source":"# Save model and tokenizer\noutput_dir = \"./bangla_roberta_sentiment_hybrid\"\nmodel.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\n\n# Save optimizer state\ntorch.save({\n    'epoch': num_epochs,\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'best_val_f1': best_val_f1,\n}, f\"{output_dir}/checkpoint.pt\")\n\nprint(f\"Model saved to {output_dir}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 13: Inference function\n","metadata":{}},{"cell_type":"code","source":"def predict_sentiment(text, model, tokenizer, device):\n    \"\"\"Predict sentiment for new text\"\"\"\n    model.eval()\n    \n    # Preprocess text\n    text = preprocess_bangla_text(text)\n    \n    # Tokenize\n    encoding = tokenizer(\n        text,\n        truncation=True,\n        padding='max_length',\n        max_length=128,\n        return_tensors='pt'\n    )\n    \n    # Move to device\n    input_ids = encoding['input_ids'].to(device)\n    attention_mask = encoding['attention_mask'].to(device)\n    \n    # Predict\n    with torch.no_grad():\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        probs = torch.softmax(logits, dim=1)\n        _, predicted = torch.max(logits, dim=1)\n    \n    # Get label\n    label_names = ['Positive', 'Negative', 'Neutral']\n    predicted_label = label_names[predicted.item()]\n    confidence = probs[0][predicted.item()].item()\n    \n    return {\n        'text': text,\n        'sentiment': predicted_label,\n        'confidence': confidence,\n        'probabilities': {\n            label_names[i]: probs[0][i].item() \n            for i in range(len(label_names))\n        }\n    }\n\n# Test the function\ntest_texts = [\n    \"এই পণ্যটি অসাধারণ\",\n    \"খুবই খারাপ সার্ভিস\",\n    \"মোটামুটি ভালো\"\n]\n\nfor text in test_texts:\n    result = predict_sentiment(text, model, tokenizer, device)\n    print(f\"\\nText: {result['text']}\")\n    print(f\"Sentiment: {result['sentiment']} (Confidence: {result['confidence']:.3f})\")\n    print(f\"All probabilities: {result['probabilities']}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}